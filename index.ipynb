{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization and Optimization Lab\n",
    "\n",
    "## Objective\n",
    "\n",
    "In this lab, we'll gain experience detecting and dealing with a ANN model that is overfitting using various regularization and hyperparameter tuning techniques!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "\n",
    "In this lab, we'll work with a large dataset of customer complaints to a bank, with the goal of predicting what product the customer is complaining about based on the text of their complaint.  There are 7 different possible products that we can predict, making this a multi-class classification task. \n",
    "\n",
    "\n",
    "#### Preprocessing our Data Set\n",
    "We'll start by preprocessing our dataset by tokenizing the complaints and limiting the number of words we consider to reduce dimensionality. \n",
    "\n",
    "#### Building our Tuning our Model\n",
    "Once we have preprocessed our data set, we'll build a model and explore the various ways that we can reduce overfitting using the following strategies:\n",
    "- Early stopping to minimize the discrepancy between train and test accuracy.\n",
    "- L1 and L2 regularization.\n",
    "- Dropout regularization.\n",
    "- Using more data.\n",
    "\n",
    "\n",
    "**_Let's Get Started!_**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocessing the Bank Complaints Data Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Import the libraries and take a sample\n",
    "\n",
    "Run the cell below to import everything we'll need for this lab. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn import preprocessing\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import random\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, in the cell below, import our data into a DataFrame.  The data is currently stored in `Bank_complaints.csv`.\n",
    "Then, `.describe()` the dataset to get a feel for what we're working with. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product</th>\n",
       "      <th>Consumer complaint narrative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>60000</td>\n",
       "      <td>60000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>7</td>\n",
       "      <td>59724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>I am filing this complaint because Experian ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>11404</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Product                       Consumer complaint narrative\n",
       "count          60000                                              60000\n",
       "unique             7                                              59724\n",
       "top     Student loan  I am filing this complaint because Experian ha...\n",
       "freq           11404                                                 26"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df =  pd.read_csv(\"Bank_complaints.csv\")\n",
    "df.describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to speed things up during the development process (and also to give us the ability to see how adding more data affects our model performance), we're going to work with a sample of our dataset rather than the whole thing.  The entire dataset consists of 60,000 rows--we're going to build a model using only 10,000 items randomly sampled from this.\n",
    "\n",
    "In the cell below:\n",
    "\n",
    "* Get a random sample of `10000` items from our dataset (HINT: use the `df` object's `.sample()` method to make this easy)\n",
    "* Reset the indexes on these samples to `range(10000)`, so that the indices for our rows are sequential and make sense.\n",
    "* Store our labels, which are found in `\"Product\"`, in a different variable.\n",
    "* Store the data, found in `\"Consumer complaint narrative`, in the variable `complaints`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(10000)\n",
    "df.index = df.reset_index()\n",
    "product = df[\"Product\"]\n",
    "complaints = df[\"Consumer complaint narrative\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Tokenizing the Complaints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll only keep 2,000 most common words and use one-hot encoding to quickly vectorize our dataset from text into a format that a neural network can work with. \n",
    "\n",
    "In the cell below:\n",
    "\n",
    "* Create a `Tokenizer()` object, and set the `num_words` parameter to `2000`.\n",
    "* Call the tokenizer object's `fit_on_texts()` method and pass in our `complaints` variable we created above. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=2000)\n",
    "tokenizer.fit_on_texts(complaints)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll create some text sequences by calling the `tokenizer` object's `.texts_to_sequences()` method and feeding in our `complaints` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = tokenizer.texts_to_sequences(complaints)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we'll convert our text data from text to a vectorized matrix.  \n",
    "\n",
    "In the cell below:\n",
    "\n",
    "* Call the `tokenizer` object's `.texts_to_matrix` method, passing in our `complaints` variable, as well as setting the `mode` parameter equal to `'binary'`.\n",
    "* Store the tokenizer's `.word_index` in the appropriate variable.\n",
    "* Check the `np.shape()` of our `one_hot_results`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 2000)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_results= tokenizer.texts_to_matrix(complaints)\n",
    "word_index = tokenizer.word_index\n",
    "np.shape(one_hot_results) # Expected Results (10000, 2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 One-hot Encoding of the Products Column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've tokenized and encoded our text data, we still need to one-hot encode our label data.  \n",
    "\n",
    "In the cell below:\n",
    "\n",
    "\n",
    "* Create a `LabelEncoder` object, which can found inside the `preprocessing` module.\n",
    "* `fit` the label encoder we just created to `product`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LabelEncoder()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LabelEncoder</label><div class=\"sk-toggleable__content\"><pre>LabelEncoder()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "le.fit(product)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check what classes our label encoder found.  Run the cell below to examine a list of classes that `product` contains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Bank account or service',\n",
       " 'Checking or savings account',\n",
       " 'Consumer Loan',\n",
       " 'Credit card',\n",
       " 'Credit reporting',\n",
       " 'Mortgage',\n",
       " 'Student loan']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " list(le.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll need to transform `product` into a numeric vector.  \n",
    "\n",
    "In the cell below, use the label encoder's `.transform` method on `product` to create an integer encoded version of our labels. \n",
    "\n",
    "Then, access `product_cat` to see an example of how the labels are now encoded. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 3, 2, ..., 0, 1, 6])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product_cat = le.transform(product)\n",
    "product_cat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to go from integer encoding to one-hot encoding.  Use the `to_categorical` method from keras to do this easily in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_onehot = to_categorical(product_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 1., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product_onehot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's check the shape of our one-hot encoded labels to make sure everything worked correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 7)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(product_onehot) # Expected Output: (10000, 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Train - test split\n",
    "\n",
    "Now, we'll split our data into training and testing sets.  \n",
    "\n",
    "\n",
    "We'll accomplish this by generating a random list of 1500 different indices between 1 and 10000.  Then, we'll slice these rows and store them as our test set, and delete them from the training set (it's very important to remember to remove them from the training set!)\n",
    "\n",
    "Run the cell below to create a set of random indices for our test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_index = random.sample(range(1,10000), 1500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now:\n",
    "\n",
    "* Slice the `test_index` rows from `one_hot_results` and store them in `test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = one_hot_results[test_index]\n",
    "\n",
    "# This line returns a version of our one_hot_results that has every item with an index in test_index removed\n",
    "train = np.delete(one_hot_results, test_index, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll need to repeat the splitting process on our labels, making sure that we use the same indices we used to split our data. \n",
    "\n",
    "In the cell below:\n",
    "\n",
    "* Slice `test_index` from `product_onehot`\n",
    "* Use `np.delete` to remove `test_index` items from `product_onehot` (the syntax is exactly the same above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_test = product_onehot[test_index]\n",
    "label_train = np.delete(product_onehot, test_index, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's examine the shape everything we just did to make sure that the dimensions match up.  \n",
    "\n",
    "In the cell below, use `np.shape` to check the shape of:\n",
    "\n",
    "* `label_test`\n",
    "* `label_train`\n",
    "* `test`\n",
    "* `train`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 1.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n",
      "[[0. 0. 0. ... 0. 1. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 1.]]\n",
      "[[0. 1. 1. ... 0. 0. 0.]\n",
      " [0. 1. 1. ... 0. 0. 0.]\n",
      " [0. 1. 1. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 1. 1. ... 0. 0. 0.]\n",
      " [0. 1. 1. ... 0. 0. 0.]\n",
      " [0. 1. 1. ... 0. 0. 0.]]\n",
      "[[0. 1. 1. ... 0. 0. 0.]\n",
      " [0. 1. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 1. 1. ... 0. 0. 0.]\n",
      " [0. 1. 1. ... 0. 0. 0.]\n",
      " [0. 1. 1. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(label_test) # Expected Output: (1500, 7)\n",
    "print(label_train) # Expected Output: (8500, 7)\n",
    "print(test) # Expected Output: (1500, 2000)\n",
    "print(train) # Expected Output: (8500, 2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Running the model using a validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Creating the validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the lecture we mentioned that in deep learning, we generally keep aside a validation set, which is used during hyperparameter tuning. Then when we have made the final model decision, the test set is used to define the final model perforance. \n",
    "\n",
    "In this example, let's take the first 1000 cases out of the training set to become the validation set. You should do this for both `train` and `label_train`.\n",
    "\n",
    "Run the cell below to create our validation set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(123)\n",
    "val = train[:1000]\n",
    "train_final = train[1000:]\n",
    "label_val = label_train[:1000]\n",
    "label_train_final = label_train[1000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Creating, compiling and running the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's rebuild a fully connected (Dense) layer network with relu activations in Keras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that we used 2 hidden with 50 units in the first layer and 25 in the second, both with a `relu` activation function. Because we are dealing with a multiclass problem (classifying the complaints into 7 classes), we use a use a softmax classifyer in order to output 7 class probabilities per case.\n",
    "\n",
    "In the cell below:\n",
    "\n",
    "* Import `Sequential` from the appropriate module in keras.\n",
    "* Import `Dense` from the appropriate module in keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras \n",
    "from keras import models\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.losses import binary_crossentropy\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(50, activation='relu', input_dim=2000))\n",
    "model.add(Dense(25, activation='relu'))\n",
    "model.add(Dense(7, activation='softmax'))\n",
    "model.compile(optimizer='SGD', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, build a model with the following specifications in the cell below:\n",
    "\n",
    "* An input layer of shape `(2000,)`\n",
    "* Hidden layer 1: Dense, 50 neurons, relu activation \n",
    "* Hidden layer 2: Dense, 25 neurons, relu activation\n",
    "* Output layer: Dense, 7 neurons, softmax activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below, `compile` the model with the following settings:\n",
    "\n",
    "* Optimizer is `\"SGD\"`\n",
    "* Loss is `'categorical_crossentropy'`\n",
    "* metrics is `['accuracy']`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.compile(optimizer=None,\n",
    "#               loss=None,\n",
    "#               metrics=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, Train the model for 120 epochs in mini-batches of 256 samples. Also pass in `(val, label_val)` to the `validation_data` parameter, so that we see how our model does on the test set after every epoch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/120\n",
      "30/30 [==============================] - 2s 36ms/step - loss: 1.9576 - accuracy: 0.1527 - val_loss: 1.9464 - val_accuracy: 0.1710\n",
      "Epoch 2/120\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 1.9251 - accuracy: 0.1908 - val_loss: 1.9249 - val_accuracy: 0.1810\n",
      "Epoch 3/120\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 1.9051 - accuracy: 0.2213 - val_loss: 1.9077 - val_accuracy: 0.2040\n",
      "Epoch 4/120\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 1.8868 - accuracy: 0.2331 - val_loss: 1.8914 - val_accuracy: 0.2170\n",
      "Epoch 5/120\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 1.8677 - accuracy: 0.2487 - val_loss: 1.8728 - val_accuracy: 0.2350\n",
      "Epoch 6/120\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 1.8460 - accuracy: 0.2647 - val_loss: 1.8525 - val_accuracy: 0.2520\n",
      "Epoch 7/120\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 1.8215 - accuracy: 0.2809 - val_loss: 1.8275 - val_accuracy: 0.2730\n",
      "Epoch 8/120\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 1.7929 - accuracy: 0.3023 - val_loss: 1.7976 - val_accuracy: 0.3010\n",
      "Epoch 9/120\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 1.7602 - accuracy: 0.3271 - val_loss: 1.7638 - val_accuracy: 0.3300\n",
      "Epoch 10/120\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 1.7234 - accuracy: 0.3589 - val_loss: 1.7256 - val_accuracy: 0.3530\n",
      "Epoch 11/120\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 1.6822 - accuracy: 0.3811 - val_loss: 1.6824 - val_accuracy: 0.3920\n",
      "Epoch 12/120\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 1.6372 - accuracy: 0.4225 - val_loss: 1.6378 - val_accuracy: 0.4080\n",
      "Epoch 13/120\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 1.5885 - accuracy: 0.4539 - val_loss: 1.5913 - val_accuracy: 0.4270\n",
      "Epoch 14/120\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 1.5373 - accuracy: 0.4764 - val_loss: 1.5375 - val_accuracy: 0.4510\n",
      "Epoch 15/120\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 1.4829 - accuracy: 0.5037 - val_loss: 1.4829 - val_accuracy: 0.4860\n",
      "Epoch 16/120\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 1.4273 - accuracy: 0.5316 - val_loss: 1.4292 - val_accuracy: 0.5120\n",
      "Epoch 17/120\n",
      "30/30 [==============================] - 1s 18ms/step - loss: 1.3718 - accuracy: 0.5567 - val_loss: 1.3769 - val_accuracy: 0.5250\n",
      "Epoch 18/120\n",
      "30/30 [==============================] - 1s 17ms/step - loss: 1.3177 - accuracy: 0.5783 - val_loss: 1.3242 - val_accuracy: 0.5520\n",
      "Epoch 19/120\n",
      "30/30 [==============================] - 1s 17ms/step - loss: 1.2655 - accuracy: 0.5984 - val_loss: 1.2761 - val_accuracy: 0.5670\n",
      "Epoch 20/120\n",
      "30/30 [==============================] - 1s 18ms/step - loss: 1.2160 - accuracy: 0.6221 - val_loss: 1.2272 - val_accuracy: 0.5860\n",
      "Epoch 21/120\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 1.1692 - accuracy: 0.6419 - val_loss: 1.1851 - val_accuracy: 0.5910\n",
      "Epoch 22/120\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 1.1257 - accuracy: 0.6600 - val_loss: 1.1476 - val_accuracy: 0.6150\n",
      "Epoch 23/120\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 1.0849 - accuracy: 0.6715 - val_loss: 1.1114 - val_accuracy: 0.6250\n",
      "Epoch 24/120\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 1.0467 - accuracy: 0.6827 - val_loss: 1.0771 - val_accuracy: 0.6290\n",
      "Epoch 25/120\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 1.0117 - accuracy: 0.6900 - val_loss: 1.0424 - val_accuracy: 0.6340\n",
      "Epoch 26/120\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 0.9788 - accuracy: 0.6991 - val_loss: 1.0168 - val_accuracy: 0.6410\n",
      "Epoch 27/120\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.9493 - accuracy: 0.7063 - val_loss: 0.9903 - val_accuracy: 0.6430\n",
      "Epoch 28/120\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 0.9218 - accuracy: 0.7145 - val_loss: 0.9657 - val_accuracy: 0.6510\n",
      "Epoch 29/120\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 0.8964 - accuracy: 0.7200 - val_loss: 0.9437 - val_accuracy: 0.6560\n",
      "Epoch 30/120\n",
      "30/30 [==============================] - 0s 15ms/step - loss: 0.8726 - accuracy: 0.7263 - val_loss: 0.9247 - val_accuracy: 0.6650\n",
      "Epoch 31/120\n",
      "30/30 [==============================] - 1s 20ms/step - loss: 0.8505 - accuracy: 0.7317 - val_loss: 0.9081 - val_accuracy: 0.6680\n",
      "Epoch 32/120\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 0.8301 - accuracy: 0.7380 - val_loss: 0.8908 - val_accuracy: 0.6800\n",
      "Epoch 33/120\n",
      "30/30 [==============================] - 0s 15ms/step - loss: 0.8117 - accuracy: 0.7447 - val_loss: 0.8760 - val_accuracy: 0.6750\n",
      "Epoch 34/120\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 0.7937 - accuracy: 0.7455 - val_loss: 0.8622 - val_accuracy: 0.6820\n",
      "Epoch 35/120\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 0.7778 - accuracy: 0.7509 - val_loss: 0.8507 - val_accuracy: 0.6850\n",
      "Epoch 36/120\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 0.7623 - accuracy: 0.7552 - val_loss: 0.8381 - val_accuracy: 0.6840\n",
      "Epoch 37/120\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 0.7477 - accuracy: 0.7585 - val_loss: 0.8259 - val_accuracy: 0.6930\n",
      "Epoch 38/120\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 0.7344 - accuracy: 0.7617 - val_loss: 0.8162 - val_accuracy: 0.6890\n",
      "Epoch 39/120\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 0.7219 - accuracy: 0.7683 - val_loss: 0.8090 - val_accuracy: 0.6930\n",
      "Epoch 40/120\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 0.7098 - accuracy: 0.7696 - val_loss: 0.7988 - val_accuracy: 0.6920\n",
      "Epoch 41/120\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 0.6985 - accuracy: 0.7724 - val_loss: 0.7913 - val_accuracy: 0.7060\n",
      "Epoch 42/120\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 0.6881 - accuracy: 0.7765 - val_loss: 0.7820 - val_accuracy: 0.7120\n",
      "Epoch 43/120\n",
      "30/30 [==============================] - 0s 15ms/step - loss: 0.6778 - accuracy: 0.7779 - val_loss: 0.7756 - val_accuracy: 0.7070\n",
      "Epoch 44/120\n",
      "30/30 [==============================] - 1s 25ms/step - loss: 0.6683 - accuracy: 0.7819 - val_loss: 0.7724 - val_accuracy: 0.7060\n",
      "Epoch 45/120\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 0.6590 - accuracy: 0.7825 - val_loss: 0.7631 - val_accuracy: 0.7090\n",
      "Epoch 46/120\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 0.6501 - accuracy: 0.7844 - val_loss: 0.7596 - val_accuracy: 0.7060\n",
      "Epoch 47/120\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 0.6415 - accuracy: 0.7859 - val_loss: 0.7561 - val_accuracy: 0.7050\n",
      "Epoch 48/120\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 0.6332 - accuracy: 0.7888 - val_loss: 0.7483 - val_accuracy: 0.7120\n",
      "Epoch 49/120\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 0.6256 - accuracy: 0.7933 - val_loss: 0.7437 - val_accuracy: 0.7020\n",
      "Epoch 50/120\n",
      "30/30 [==============================] - 1s 23ms/step - loss: 0.6180 - accuracy: 0.7952 - val_loss: 0.7388 - val_accuracy: 0.7160\n",
      "Epoch 51/120\n",
      "30/30 [==============================] - 1s 29ms/step - loss: 0.6109 - accuracy: 0.7981 - val_loss: 0.7345 - val_accuracy: 0.7150\n",
      "Epoch 52/120\n",
      "30/30 [==============================] - 1s 27ms/step - loss: 0.6037 - accuracy: 0.7992 - val_loss: 0.7307 - val_accuracy: 0.7150\n",
      "Epoch 53/120\n",
      "30/30 [==============================] - 1s 26ms/step - loss: 0.5971 - accuracy: 0.8021 - val_loss: 0.7273 - val_accuracy: 0.7230\n",
      "Epoch 54/120\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 0.5902 - accuracy: 0.8031 - val_loss: 0.7229 - val_accuracy: 0.7140\n",
      "Epoch 55/120\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 0.5842 - accuracy: 0.8065 - val_loss: 0.7236 - val_accuracy: 0.7160\n",
      "Epoch 56/120\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.5780 - accuracy: 0.8069 - val_loss: 0.7242 - val_accuracy: 0.7190\n",
      "Epoch 57/120\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 0.5723 - accuracy: 0.8103 - val_loss: 0.7183 - val_accuracy: 0.7150\n",
      "Epoch 58/120\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 0.5665 - accuracy: 0.8116 - val_loss: 0.7120 - val_accuracy: 0.7210\n",
      "Epoch 59/120\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 0.5604 - accuracy: 0.8132 - val_loss: 0.7162 - val_accuracy: 0.7110\n",
      "Epoch 60/120\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 0.5556 - accuracy: 0.8121 - val_loss: 0.7088 - val_accuracy: 0.7180\n",
      "Epoch 61/120\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 0.5499 - accuracy: 0.8148 - val_loss: 0.7045 - val_accuracy: 0.7260\n",
      "Epoch 62/120\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 0.5448 - accuracy: 0.8209 - val_loss: 0.7019 - val_accuracy: 0.7240\n",
      "Epoch 63/120\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.5396 - accuracy: 0.8201 - val_loss: 0.7042 - val_accuracy: 0.7200\n",
      "Epoch 64/120\n",
      "30/30 [==============================] - 1s 23ms/step - loss: 0.5352 - accuracy: 0.8220 - val_loss: 0.6967 - val_accuracy: 0.7260\n",
      "Epoch 65/120\n",
      "30/30 [==============================] - 1s 22ms/step - loss: 0.5304 - accuracy: 0.8249 - val_loss: 0.6976 - val_accuracy: 0.7220\n",
      "Epoch 66/120\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 0.5254 - accuracy: 0.8241 - val_loss: 0.6952 - val_accuracy: 0.7240\n",
      "Epoch 67/120\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 0.5205 - accuracy: 0.8279 - val_loss: 0.6961 - val_accuracy: 0.7210\n",
      "Epoch 68/120\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 0.5165 - accuracy: 0.8275 - val_loss: 0.6936 - val_accuracy: 0.7340\n",
      "Epoch 69/120\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 0.5120 - accuracy: 0.8301 - val_loss: 0.6915 - val_accuracy: 0.7270\n",
      "Epoch 70/120\n",
      "30/30 [==============================] - 1s 28ms/step - loss: 0.5073 - accuracy: 0.8312 - val_loss: 0.6883 - val_accuracy: 0.7280\n",
      "Epoch 71/120\n",
      "30/30 [==============================] - 1s 28ms/step - loss: 0.5034 - accuracy: 0.8316 - val_loss: 0.6891 - val_accuracy: 0.7340\n",
      "Epoch 72/120\n",
      "30/30 [==============================] - 1s 20ms/step - loss: 0.4991 - accuracy: 0.8349 - val_loss: 0.6853 - val_accuracy: 0.7260\n",
      "Epoch 73/120\n",
      "30/30 [==============================] - 1s 18ms/step - loss: 0.4947 - accuracy: 0.8327 - val_loss: 0.6886 - val_accuracy: 0.7270\n",
      "Epoch 74/120\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 0.4910 - accuracy: 0.8369 - val_loss: 0.6835 - val_accuracy: 0.7280\n",
      "Epoch 75/120\n",
      "30/30 [==============================] - 0s 15ms/step - loss: 0.4866 - accuracy: 0.8368 - val_loss: 0.6859 - val_accuracy: 0.7340\n",
      "Epoch 76/120\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 0.4830 - accuracy: 0.8392 - val_loss: 0.6833 - val_accuracy: 0.7290\n",
      "Epoch 77/120\n",
      "30/30 [==============================] - 1s 19ms/step - loss: 0.4792 - accuracy: 0.8408 - val_loss: 0.6799 - val_accuracy: 0.7330\n",
      "Epoch 78/120\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 0.4748 - accuracy: 0.8427 - val_loss: 0.6812 - val_accuracy: 0.7290\n",
      "Epoch 79/120\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.4719 - accuracy: 0.8421 - val_loss: 0.6788 - val_accuracy: 0.7260\n",
      "Epoch 80/120\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 0.4676 - accuracy: 0.8435 - val_loss: 0.6778 - val_accuracy: 0.7340\n",
      "Epoch 81/120\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 0.4642 - accuracy: 0.8437 - val_loss: 0.6800 - val_accuracy: 0.7260\n",
      "Epoch 82/120\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 0.4606 - accuracy: 0.8468 - val_loss: 0.6754 - val_accuracy: 0.7390\n",
      "Epoch 83/120\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 0.4571 - accuracy: 0.8488 - val_loss: 0.6767 - val_accuracy: 0.7290\n",
      "Epoch 84/120\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 0.4535 - accuracy: 0.8495 - val_loss: 0.6740 - val_accuracy: 0.7370\n",
      "Epoch 85/120\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 0.4504 - accuracy: 0.8504 - val_loss: 0.6731 - val_accuracy: 0.7380\n",
      "Epoch 86/120\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.4469 - accuracy: 0.8521 - val_loss: 0.6713 - val_accuracy: 0.7340\n",
      "Epoch 87/120\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.4436 - accuracy: 0.8516 - val_loss: 0.6725 - val_accuracy: 0.7380\n",
      "Epoch 88/120\n",
      "30/30 [==============================] - 0s 15ms/step - loss: 0.4406 - accuracy: 0.8529 - val_loss: 0.6725 - val_accuracy: 0.7410\n",
      "Epoch 89/120\n",
      "30/30 [==============================] - 1s 19ms/step - loss: 0.4367 - accuracy: 0.8552 - val_loss: 0.6747 - val_accuracy: 0.7320\n",
      "Epoch 90/120\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 0.4337 - accuracy: 0.8560 - val_loss: 0.6695 - val_accuracy: 0.7390\n",
      "Epoch 91/120\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.4297 - accuracy: 0.8608 - val_loss: 0.6780 - val_accuracy: 0.7430\n",
      "Epoch 92/120\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.4280 - accuracy: 0.8579 - val_loss: 0.6774 - val_accuracy: 0.7440\n",
      "Epoch 93/120\n",
      "30/30 [==============================] - 0s 16ms/step - loss: 0.4246 - accuracy: 0.8612 - val_loss: 0.6699 - val_accuracy: 0.7480\n",
      "Epoch 94/120\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.4212 - accuracy: 0.8611 - val_loss: 0.6725 - val_accuracy: 0.7510\n",
      "Epoch 95/120\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 0.4180 - accuracy: 0.8609 - val_loss: 0.6720 - val_accuracy: 0.7380\n",
      "Epoch 96/120\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.4151 - accuracy: 0.8653 - val_loss: 0.6671 - val_accuracy: 0.7460\n",
      "Epoch 97/120\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.4120 - accuracy: 0.8636 - val_loss: 0.6682 - val_accuracy: 0.7430\n",
      "Epoch 98/120\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 0.4092 - accuracy: 0.8661 - val_loss: 0.6677 - val_accuracy: 0.7410\n",
      "Epoch 99/120\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 0.4065 - accuracy: 0.8672 - val_loss: 0.6711 - val_accuracy: 0.7400\n",
      "Epoch 100/120\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.4031 - accuracy: 0.8696 - val_loss: 0.6672 - val_accuracy: 0.7480\n",
      "Epoch 101/120\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 0.4005 - accuracy: 0.8703 - val_loss: 0.6695 - val_accuracy: 0.7400\n",
      "Epoch 102/120\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 0.3975 - accuracy: 0.8704 - val_loss: 0.6681 - val_accuracy: 0.7480\n",
      "Epoch 103/120\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 0.3946 - accuracy: 0.8712 - val_loss: 0.6714 - val_accuracy: 0.7380\n",
      "Epoch 104/120\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 0.3918 - accuracy: 0.8736 - val_loss: 0.6661 - val_accuracy: 0.7390\n",
      "Epoch 105/120\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 0.3892 - accuracy: 0.8735 - val_loss: 0.6676 - val_accuracy: 0.7500\n",
      "Epoch 106/120\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 0.3864 - accuracy: 0.8752 - val_loss: 0.6703 - val_accuracy: 0.7360\n",
      "Epoch 107/120\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 0.3842 - accuracy: 0.8763 - val_loss: 0.6682 - val_accuracy: 0.7470\n",
      "Epoch 108/120\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 0.3815 - accuracy: 0.8768 - val_loss: 0.6683 - val_accuracy: 0.7390\n",
      "Epoch 109/120\n",
      "30/30 [==============================] - 0s 15ms/step - loss: 0.3781 - accuracy: 0.8780 - val_loss: 0.6693 - val_accuracy: 0.7450\n",
      "Epoch 110/120\n",
      "30/30 [==============================] - 1s 26ms/step - loss: 0.3758 - accuracy: 0.8789 - val_loss: 0.6673 - val_accuracy: 0.7350\n",
      "Epoch 111/120\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 0.3733 - accuracy: 0.8817 - val_loss: 0.6662 - val_accuracy: 0.7490\n",
      "Epoch 112/120\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.3710 - accuracy: 0.8811 - val_loss: 0.6672 - val_accuracy: 0.7440\n",
      "Epoch 113/120\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 0.3683 - accuracy: 0.8823 - val_loss: 0.6766 - val_accuracy: 0.7310\n",
      "Epoch 114/120\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 0.3660 - accuracy: 0.8833 - val_loss: 0.6705 - val_accuracy: 0.7350\n",
      "Epoch 115/120\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 0.3631 - accuracy: 0.8867 - val_loss: 0.6688 - val_accuracy: 0.7360\n",
      "Epoch 116/120\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 0.3606 - accuracy: 0.8859 - val_loss: 0.6730 - val_accuracy: 0.7380\n",
      "Epoch 117/120\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 0.3583 - accuracy: 0.8864 - val_loss: 0.6686 - val_accuracy: 0.7440\n",
      "Epoch 118/120\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 0.3561 - accuracy: 0.8893 - val_loss: 0.6698 - val_accuracy: 0.7320\n",
      "Epoch 119/120\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 0.3530 - accuracy: 0.8887 - val_loss: 0.6742 - val_accuracy: 0.7480\n",
      "Epoch 120/120\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 0.3512 - accuracy: 0.8897 - val_loss: 0.6764 - val_accuracy: 0.7390\n"
     ]
    }
   ],
   "source": [
    "model_val = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=120,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dictionary `history` contains four entries this time: one per metric that was being monitored during training and during validation.\n",
    "\n",
    "In the cell below:\n",
    "\n",
    "* Store the model's `.history` inside of `model_val_dict`\n",
    "* Check what `keys()` this dictionary contains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_val_dict = model_val.history\n",
    "model_val_dict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's get the final results on the training and testing sets using `model.evaluate()` on `train_final` and `label_train_final`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235/235 [==============================] - 0s 2ms/step - loss: 0.3485 - accuracy: 0.8903\n"
     ]
    }
   ],
   "source": [
    "\n",
    "results_train = model.evaluate(train_final, label_train_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also use this function to get the results on our testing set.  Call the function again, but this time on `test` and `label_test`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47/47 [==============================] - 0s 2ms/step - loss: 0.6396 - accuracy: 0.7547\n"
     ]
    }
   ],
   "source": [
    "results_test = model.evaluate(test, label_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, check the contents of each. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.3485262095928192, 0.8902666568756104]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train # Expected Results: [0.33576024494171142, 0.89600000000000002]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6395717263221741, 0.7546666860580444]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test # Expected Results: [0.72006658554077152, 0.74333333365122478]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the results. Let's include the training and the validation loss in the same plot. We'll do the same thing for the training and validation accuracy.\n",
    "\n",
    "Run the cell below to visualize a plot of our training and validation loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "loss_values = model_val_dict['loss']\n",
    "val_loss_values = model_val_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(loss_values) + 1)\n",
    "plt.plot(epochs, loss_values, 'g', label='Training loss')\n",
    "plt.plot(epochs, val_loss_values, 'g.', label='Validation loss')\n",
    "\n",
    "plt.title('Training & validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting! \n",
    "\n",
    "Run the cell below to visualize a plot of our training and validation accuracy>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "\n",
    "acc_values = model_val_dict['accuracy'] \n",
    "val_acc_values = model_val_dict['val_accuracy']\n",
    "\n",
    "plt.plot(epochs, acc_values, 'r', label='Training acc')\n",
    "plt.plot(epochs, val_acc_values, 'r.', label='Validation acc')\n",
    "plt.title('Training & validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe an interesting pattern here: although the training accuracy keeps increasing when going through more epochs, and the training loss keeps decreasing, the validation accuracy and loss seem to be reaching a status quo around the 60th epoch. This means that we're actually **overfitting** to the train data when we do as many epochs as we were doing. Luckily, you learned how to tackle overfitting in the previous lecture! For starters, it does seem clear that we are training too long. So let's stop training at the 60th epoch first (so-called \"early stopping\") before we move to more advanced regularization techniques!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Early stopping\n",
    "\n",
    "Now that we know that the model starts to overfit around epoch 60, we can just retrain the model from scratch, but this time only up to 60 epochs! This will help us with our overfitting problem.  This method is called **_Early Stopping_**.\n",
    "\n",
    "In the cell below: \n",
    "\n",
    "* Recreate the exact model we did above. \n",
    "* Compile the model with the exact same hyperparameters.\n",
    "* Fit the model with the exact same hyperparameters, with the exception of `epochs`.  This time, set epochs to `60` instead of `120`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "30/30 [==============================] - 1s 18ms/step - loss: 1.9477 - accuracy: 0.1500 - val_loss: 1.9454 - val_accuracy: 0.1540\n",
      "Epoch 2/60\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 1.9233 - accuracy: 0.1895 - val_loss: 1.9275 - val_accuracy: 0.1880\n",
      "Epoch 3/60\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 1.9032 - accuracy: 0.2251 - val_loss: 1.9108 - val_accuracy: 0.2060\n",
      "Epoch 4/60\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 1.8826 - accuracy: 0.2521 - val_loss: 1.8919 - val_accuracy: 0.2290\n",
      "Epoch 5/60\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 1.8589 - accuracy: 0.2747 - val_loss: 1.8698 - val_accuracy: 0.2560\n",
      "Epoch 6/60\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 1.8310 - accuracy: 0.2988 - val_loss: 1.8433 - val_accuracy: 0.2710\n",
      "Epoch 7/60\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 1.7992 - accuracy: 0.3251 - val_loss: 1.8122 - val_accuracy: 0.2950\n",
      "Epoch 8/60\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 1.7624 - accuracy: 0.3429 - val_loss: 1.7748 - val_accuracy: 0.3200\n",
      "Epoch 9/60\n",
      "30/30 [==============================] - 1s 30ms/step - loss: 1.7202 - accuracy: 0.3605 - val_loss: 1.7310 - val_accuracy: 0.3500\n",
      "Epoch 10/60\n",
      "30/30 [==============================] - 1s 31ms/step - loss: 1.6731 - accuracy: 0.3847 - val_loss: 1.6828 - val_accuracy: 0.3760\n",
      "Epoch 11/60\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 1.6232 - accuracy: 0.4041 - val_loss: 1.6329 - val_accuracy: 0.4000\n",
      "Epoch 12/60\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 1.5713 - accuracy: 0.4328 - val_loss: 1.5812 - val_accuracy: 0.4220\n",
      "Epoch 13/60\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 1.5172 - accuracy: 0.4677 - val_loss: 1.5287 - val_accuracy: 0.4470\n",
      "Epoch 14/60\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 1.4628 - accuracy: 0.4945 - val_loss: 1.4741 - val_accuracy: 0.4920\n",
      "Epoch 15/60\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 1.4080 - accuracy: 0.5279 - val_loss: 1.4205 - val_accuracy: 0.5190\n",
      "Epoch 16/60\n",
      "30/30 [==============================] - 1s 19ms/step - loss: 1.3542 - accuracy: 0.5624 - val_loss: 1.3688 - val_accuracy: 0.5500\n",
      "Epoch 17/60\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 1.3019 - accuracy: 0.5869 - val_loss: 1.3208 - val_accuracy: 0.5560\n",
      "Epoch 18/60\n",
      "30/30 [==============================] - 1s 24ms/step - loss: 1.2520 - accuracy: 0.6041 - val_loss: 1.2730 - val_accuracy: 0.5740\n",
      "Epoch 19/60\n",
      "30/30 [==============================] - 1s 27ms/step - loss: 1.2046 - accuracy: 0.6249 - val_loss: 1.2291 - val_accuracy: 0.5940\n",
      "Epoch 20/60\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 1.1591 - accuracy: 0.6429 - val_loss: 1.1880 - val_accuracy: 0.6150\n",
      "Epoch 21/60\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 1.1166 - accuracy: 0.6507 - val_loss: 1.1491 - val_accuracy: 0.6200\n",
      "Epoch 22/60\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 1.0763 - accuracy: 0.6652 - val_loss: 1.1125 - val_accuracy: 0.6330\n",
      "Epoch 23/60\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 1.0392 - accuracy: 0.6764 - val_loss: 1.0788 - val_accuracy: 0.6420\n",
      "Epoch 24/60\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 1.0041 - accuracy: 0.6844 - val_loss: 1.0485 - val_accuracy: 0.6420\n",
      "Epoch 25/60\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.9718 - accuracy: 0.6949 - val_loss: 1.0189 - val_accuracy: 0.6590\n",
      "Epoch 26/60\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.9422 - accuracy: 0.7012 - val_loss: 0.9943 - val_accuracy: 0.6570\n",
      "Epoch 27/60\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.9143 - accuracy: 0.7108 - val_loss: 0.9718 - val_accuracy: 0.6680\n",
      "Epoch 28/60\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 0.8890 - accuracy: 0.7163 - val_loss: 0.9483 - val_accuracy: 0.6740\n",
      "Epoch 29/60\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.8653 - accuracy: 0.7233 - val_loss: 0.9285 - val_accuracy: 0.6770\n",
      "Epoch 30/60\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.8436 - accuracy: 0.7295 - val_loss: 0.9101 - val_accuracy: 0.6820\n",
      "Epoch 31/60\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 0.8234 - accuracy: 0.7356 - val_loss: 0.8924 - val_accuracy: 0.6920\n",
      "Epoch 32/60\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 0.8047 - accuracy: 0.7399 - val_loss: 0.8791 - val_accuracy: 0.6880\n",
      "Epoch 33/60\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.7877 - accuracy: 0.7463 - val_loss: 0.8635 - val_accuracy: 0.6930\n",
      "Epoch 34/60\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.7713 - accuracy: 0.7471 - val_loss: 0.8493 - val_accuracy: 0.6940\n",
      "Epoch 35/60\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.7562 - accuracy: 0.7536 - val_loss: 0.8380 - val_accuracy: 0.6950\n",
      "Epoch 36/60\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.7419 - accuracy: 0.7552 - val_loss: 0.8291 - val_accuracy: 0.7050\n",
      "Epoch 37/60\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.7288 - accuracy: 0.7592 - val_loss: 0.8170 - val_accuracy: 0.7020\n",
      "Epoch 38/60\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.7167 - accuracy: 0.7612 - val_loss: 0.8099 - val_accuracy: 0.7000\n",
      "Epoch 39/60\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.7049 - accuracy: 0.7664 - val_loss: 0.8002 - val_accuracy: 0.7060\n",
      "Epoch 40/60\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.6935 - accuracy: 0.7688 - val_loss: 0.7960 - val_accuracy: 0.7120\n",
      "Epoch 41/60\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.6835 - accuracy: 0.7704 - val_loss: 0.7882 - val_accuracy: 0.7160\n",
      "Epoch 42/60\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.6736 - accuracy: 0.7743 - val_loss: 0.7744 - val_accuracy: 0.7080\n",
      "Epoch 43/60\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.6639 - accuracy: 0.7765 - val_loss: 0.7695 - val_accuracy: 0.7130\n",
      "Epoch 44/60\n",
      "30/30 [==============================] - 1s 21ms/step - loss: 0.6550 - accuracy: 0.7784 - val_loss: 0.7648 - val_accuracy: 0.7130\n",
      "Epoch 45/60\n",
      "30/30 [==============================] - 1s 29ms/step - loss: 0.6461 - accuracy: 0.7803 - val_loss: 0.7611 - val_accuracy: 0.7070\n",
      "Epoch 46/60\n",
      "30/30 [==============================] - 1s 30ms/step - loss: 0.6379 - accuracy: 0.7836 - val_loss: 0.7526 - val_accuracy: 0.7150\n",
      "Epoch 47/60\n",
      "30/30 [==============================] - 1s 22ms/step - loss: 0.6300 - accuracy: 0.7855 - val_loss: 0.7569 - val_accuracy: 0.7220\n",
      "Epoch 48/60\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.6227 - accuracy: 0.7892 - val_loss: 0.7443 - val_accuracy: 0.7220\n",
      "Epoch 49/60\n",
      "30/30 [==============================] - 1s 17ms/step - loss: 0.6150 - accuracy: 0.7904 - val_loss: 0.7421 - val_accuracy: 0.7260\n",
      "Epoch 50/60\n",
      "30/30 [==============================] - 1s 24ms/step - loss: 0.6083 - accuracy: 0.7933 - val_loss: 0.7421 - val_accuracy: 0.7170\n",
      "Epoch 51/60\n",
      "30/30 [==============================] - 1s 19ms/step - loss: 0.6017 - accuracy: 0.7985 - val_loss: 0.7336 - val_accuracy: 0.7280\n",
      "Epoch 52/60\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 0.5947 - accuracy: 0.7992 - val_loss: 0.7257 - val_accuracy: 0.7250\n",
      "Epoch 53/60\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 0.5885 - accuracy: 0.7985 - val_loss: 0.7241 - val_accuracy: 0.7310\n",
      "Epoch 54/60\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 0.5817 - accuracy: 0.8027 - val_loss: 0.7221 - val_accuracy: 0.7340\n",
      "Epoch 55/60\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 0.5760 - accuracy: 0.8039 - val_loss: 0.7250 - val_accuracy: 0.7310\n",
      "Epoch 56/60\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 0.5700 - accuracy: 0.8048 - val_loss: 0.7150 - val_accuracy: 0.7260\n",
      "Epoch 57/60\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.5646 - accuracy: 0.8077 - val_loss: 0.7149 - val_accuracy: 0.7280\n",
      "Epoch 58/60\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 0.5588 - accuracy: 0.8085 - val_loss: 0.7092 - val_accuracy: 0.7300\n",
      "Epoch 59/60\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 0.5534 - accuracy: 0.8119 - val_loss: 0.7105 - val_accuracy: 0.7280\n",
      "Epoch 60/60\n",
      "30/30 [==============================] - 0s 17ms/step - loss: 0.5482 - accuracy: 0.8123 - val_loss: 0.7096 - val_accuracy: 0.7340\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(50, activation='relu', input_dim=2000))\n",
    "model.add(Dense(25, activation='relu'))\n",
    "model.add(Dense(7, activation='softmax'))\n",
    "model.compile(optimizer='SGD', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model_val = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=60,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, as we did before, get our results using `model.evaluate()` on the appropriate variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235/235 [==============================] - 0s 2ms/step - loss: 0.5438 - accuracy: 0.8115\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47/47 [==============================] - 0s 3ms/step - loss: 0.6946 - accuracy: 0.7500\n"
     ]
    }
   ],
   "source": [
    "results_test = model.evaluate(test, label_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5437793731689453, 0.8114666938781738]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train  # Expected Output: [0.58606486314137773, 0.79826666669845581]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6945784687995911, 0.75]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test # [0.74768974288304646, 0.71333333365122475]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've significantly reduced the variance, so this is already pretty good! Our test set accuracy is slightly worse, but this model will definitely be more robust than the 120 epochs one we fitted before.\n",
    "\n",
    "Now, let's see what else we can do to improve the result!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. L2 regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's include L2 regularization. You can easily do this in keras adding the argument `kernel_regulizers.l2` and adding a value for the regularization parameter lambda between parentheses.\n",
    "\n",
    "In the cell below: \n",
    "\n",
    "* Recreate the same model we did before.\n",
    "* In our two hidden layers (but not our output layer), add in the parameter `kernel_regularizer=regularizers.l2(0.005)` to add L2 regularization to each hidden layer.  \n",
    "* Compile the model with the same hyperparameters as we did before. \n",
    "* Fit the model with the same hyperparameters as we did before, but this time for `120` epochs.\n",
    "* Store the fitted model that the `.fit` call returns inside a variable called `L2_model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/120\n",
      "30/30 [==============================] - 2s 29ms/step - loss: 2.6276 - accuracy: 0.1252 - val_loss: 2.5967 - val_accuracy: 0.1440\n",
      "Epoch 2/120\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 2.5875 - accuracy: 0.1680 - val_loss: 2.5766 - val_accuracy: 0.1880\n",
      "Epoch 3/120\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 2.5647 - accuracy: 0.2047 - val_loss: 2.5589 - val_accuracy: 0.2140\n",
      "Epoch 4/120\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 2.5427 - accuracy: 0.2325 - val_loss: 2.5389 - val_accuracy: 0.2320\n",
      "Epoch 5/120\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 2.5182 - accuracy: 0.2480 - val_loss: 2.5144 - val_accuracy: 0.2440\n",
      "Epoch 6/120\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 2.4883 - accuracy: 0.2697 - val_loss: 2.4829 - val_accuracy: 0.2790\n",
      "Epoch 7/120\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 2.4519 - accuracy: 0.3045 - val_loss: 2.4447 - val_accuracy: 0.3110\n",
      "Epoch 8/120\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 2.4078 - accuracy: 0.3383 - val_loss: 2.3972 - val_accuracy: 0.3640\n",
      "Epoch 9/120\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 2.3549 - accuracy: 0.3835 - val_loss: 2.3414 - val_accuracy: 0.3980\n",
      "Epoch 10/120\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 2.2931 - accuracy: 0.4241 - val_loss: 2.2784 - val_accuracy: 0.4160\n",
      "Epoch 11/120\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 2.2267 - accuracy: 0.4661 - val_loss: 2.2132 - val_accuracy: 0.4670\n",
      "Epoch 12/120\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 2.1591 - accuracy: 0.4969 - val_loss: 2.1491 - val_accuracy: 0.4980\n",
      "Epoch 13/120\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 2.0921 - accuracy: 0.5300 - val_loss: 2.0825 - val_accuracy: 0.5350\n",
      "Epoch 14/120\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 2.0266 - accuracy: 0.5625 - val_loss: 2.0201 - val_accuracy: 0.5540\n",
      "Epoch 15/120\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 1.9649 - accuracy: 0.5903 - val_loss: 1.9639 - val_accuracy: 0.5640\n",
      "Epoch 16/120\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 1.9073 - accuracy: 0.6052 - val_loss: 1.9098 - val_accuracy: 0.5790\n",
      "Epoch 17/120\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 1.8538 - accuracy: 0.6213 - val_loss: 1.8603 - val_accuracy: 0.6050\n",
      "Epoch 18/120\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 1.8041 - accuracy: 0.6421 - val_loss: 1.8141 - val_accuracy: 0.6180\n",
      "Epoch 19/120\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 1.7582 - accuracy: 0.6527 - val_loss: 1.7733 - val_accuracy: 0.6320\n",
      "Epoch 20/120\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 1.7160 - accuracy: 0.6651 - val_loss: 1.7372 - val_accuracy: 0.6360\n",
      "Epoch 21/120\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 1.6770 - accuracy: 0.6771 - val_loss: 1.6997 - val_accuracy: 0.6450\n",
      "Epoch 22/120\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 1.6415 - accuracy: 0.6873 - val_loss: 1.6656 - val_accuracy: 0.6600\n",
      "Epoch 23/120\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 1.6080 - accuracy: 0.6953 - val_loss: 1.6372 - val_accuracy: 0.6610\n",
      "Epoch 24/120\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 1.5774 - accuracy: 0.7041 - val_loss: 1.6117 - val_accuracy: 0.6740\n",
      "Epoch 25/120\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 1.5490 - accuracy: 0.7109 - val_loss: 1.5868 - val_accuracy: 0.6720\n",
      "Epoch 26/120\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 1.5224 - accuracy: 0.7175 - val_loss: 1.5614 - val_accuracy: 0.6730\n",
      "Epoch 27/120\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 1.4978 - accuracy: 0.7231 - val_loss: 1.5414 - val_accuracy: 0.6820\n",
      "Epoch 28/120\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 1.4748 - accuracy: 0.7279 - val_loss: 1.5240 - val_accuracy: 0.6840\n",
      "Epoch 29/120\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 1.4532 - accuracy: 0.7361 - val_loss: 1.5044 - val_accuracy: 0.6900\n",
      "Epoch 30/120\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 1.4330 - accuracy: 0.7365 - val_loss: 1.4885 - val_accuracy: 0.6930\n",
      "Epoch 31/120\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 1.4139 - accuracy: 0.7428 - val_loss: 1.4711 - val_accuracy: 0.6910\n",
      "Epoch 32/120\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 1.3963 - accuracy: 0.7444 - val_loss: 1.4564 - val_accuracy: 0.7020\n",
      "Epoch 33/120\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 1.3793 - accuracy: 0.7496 - val_loss: 1.4409 - val_accuracy: 0.6980\n",
      "Epoch 34/120\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 1.3632 - accuracy: 0.7543 - val_loss: 1.4299 - val_accuracy: 0.7040\n",
      "Epoch 35/120\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 1.3484 - accuracy: 0.7573 - val_loss: 1.4176 - val_accuracy: 0.7020\n",
      "Epoch 36/120\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 1.3335 - accuracy: 0.7581 - val_loss: 1.4054 - val_accuracy: 0.7030\n",
      "Epoch 37/120\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 1.3199 - accuracy: 0.7612 - val_loss: 1.3928 - val_accuracy: 0.7040\n",
      "Epoch 38/120\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 1.3069 - accuracy: 0.7643 - val_loss: 1.3832 - val_accuracy: 0.7060\n",
      "Epoch 39/120\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 1.2943 - accuracy: 0.7645 - val_loss: 1.3748 - val_accuracy: 0.7120\n",
      "Epoch 40/120\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 1.2818 - accuracy: 0.7695 - val_loss: 1.3633 - val_accuracy: 0.7110\n",
      "Epoch 41/120\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 1.2707 - accuracy: 0.7687 - val_loss: 1.3569 - val_accuracy: 0.7140\n",
      "Epoch 42/120\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 1.2594 - accuracy: 0.7725 - val_loss: 1.3478 - val_accuracy: 0.7140\n",
      "Epoch 43/120\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 1.2491 - accuracy: 0.7740 - val_loss: 1.3372 - val_accuracy: 0.7230\n",
      "Epoch 44/120\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 1.2386 - accuracy: 0.7792 - val_loss: 1.3302 - val_accuracy: 0.7230\n",
      "Epoch 45/120\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 1.2288 - accuracy: 0.7816 - val_loss: 1.3219 - val_accuracy: 0.7220\n",
      "Epoch 46/120\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 1.2191 - accuracy: 0.7816 - val_loss: 1.3173 - val_accuracy: 0.7200\n",
      "Epoch 47/120\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 1.2100 - accuracy: 0.7840 - val_loss: 1.3105 - val_accuracy: 0.7190\n",
      "Epoch 48/120\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 1.2009 - accuracy: 0.7868 - val_loss: 1.3044 - val_accuracy: 0.7240\n",
      "Epoch 49/120\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 1.1923 - accuracy: 0.7887 - val_loss: 1.2964 - val_accuracy: 0.7300\n",
      "Epoch 50/120\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 1.1838 - accuracy: 0.7919 - val_loss: 1.2908 - val_accuracy: 0.7310\n",
      "Epoch 51/120\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 1.1756 - accuracy: 0.7948 - val_loss: 1.2864 - val_accuracy: 0.7310\n",
      "Epoch 52/120\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 1.1676 - accuracy: 0.7961 - val_loss: 1.2774 - val_accuracy: 0.7400\n",
      "Epoch 53/120\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 1.1596 - accuracy: 0.7999 - val_loss: 1.2722 - val_accuracy: 0.7390\n",
      "Epoch 54/120\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 1.1518 - accuracy: 0.8012 - val_loss: 1.2675 - val_accuracy: 0.7420\n",
      "Epoch 55/120\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 1.1445 - accuracy: 0.8016 - val_loss: 1.2646 - val_accuracy: 0.7370\n",
      "Epoch 56/120\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 1.1378 - accuracy: 0.8033 - val_loss: 1.2583 - val_accuracy: 0.7410\n",
      "Epoch 57/120\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 1.1295 - accuracy: 0.8071 - val_loss: 1.2590 - val_accuracy: 0.7350\n",
      "Epoch 58/120\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 1.1235 - accuracy: 0.8064 - val_loss: 1.2476 - val_accuracy: 0.7400\n",
      "Epoch 59/120\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 1.1164 - accuracy: 0.8104 - val_loss: 1.2457 - val_accuracy: 0.7430\n",
      "Epoch 60/120\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 1.1096 - accuracy: 0.8084 - val_loss: 1.2388 - val_accuracy: 0.7440\n",
      "Epoch 61/120\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 1.1031 - accuracy: 0.8124 - val_loss: 1.2399 - val_accuracy: 0.7420\n",
      "Epoch 62/120\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 1.0968 - accuracy: 0.8116 - val_loss: 1.2280 - val_accuracy: 0.7500\n",
      "Epoch 63/120\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 1.0903 - accuracy: 0.8143 - val_loss: 1.2257 - val_accuracy: 0.7460\n",
      "Epoch 64/120\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 1.0840 - accuracy: 0.8149 - val_loss: 1.2223 - val_accuracy: 0.7420\n",
      "Epoch 65/120\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 1.0782 - accuracy: 0.8167 - val_loss: 1.2201 - val_accuracy: 0.7510\n",
      "Epoch 66/120\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 1.0718 - accuracy: 0.8215 - val_loss: 1.2158 - val_accuracy: 0.7490\n",
      "Epoch 67/120\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 1.0663 - accuracy: 0.8231 - val_loss: 1.2078 - val_accuracy: 0.7520\n",
      "Epoch 68/120\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 1.0604 - accuracy: 0.8233 - val_loss: 1.2058 - val_accuracy: 0.7510\n",
      "Epoch 69/120\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 1.0544 - accuracy: 0.8255 - val_loss: 1.2014 - val_accuracy: 0.7480\n",
      "Epoch 70/120\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 1.0490 - accuracy: 0.8256 - val_loss: 1.1981 - val_accuracy: 0.7490\n",
      "Epoch 71/120\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 1.0435 - accuracy: 0.8299 - val_loss: 1.1982 - val_accuracy: 0.7510\n",
      "Epoch 72/120\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 1.0380 - accuracy: 0.8281 - val_loss: 1.1915 - val_accuracy: 0.7520\n",
      "Epoch 73/120\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 1.0328 - accuracy: 0.8307 - val_loss: 1.1891 - val_accuracy: 0.7540\n",
      "Epoch 74/120\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 1.0272 - accuracy: 0.8309 - val_loss: 1.1898 - val_accuracy: 0.7530\n",
      "Epoch 75/120\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 1.0220 - accuracy: 0.8351 - val_loss: 1.1811 - val_accuracy: 0.7520\n",
      "Epoch 76/120\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 1.0171 - accuracy: 0.8349 - val_loss: 1.1784 - val_accuracy: 0.7600\n",
      "Epoch 77/120\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 1.0124 - accuracy: 0.8359 - val_loss: 1.1756 - val_accuracy: 0.7630\n",
      "Epoch 78/120\n",
      "30/30 [==============================] - 1s 22ms/step - loss: 1.0070 - accuracy: 0.8372 - val_loss: 1.1768 - val_accuracy: 0.7520\n",
      "Epoch 79/120\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 1.0019 - accuracy: 0.8389 - val_loss: 1.1714 - val_accuracy: 0.7620\n",
      "Epoch 80/120\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.9971 - accuracy: 0.8400 - val_loss: 1.1679 - val_accuracy: 0.7590\n",
      "Epoch 81/120\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.9923 - accuracy: 0.8404 - val_loss: 1.1645 - val_accuracy: 0.7640\n",
      "Epoch 82/120\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.9877 - accuracy: 0.8427 - val_loss: 1.1632 - val_accuracy: 0.7580\n",
      "Epoch 83/120\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.9830 - accuracy: 0.8417 - val_loss: 1.1605 - val_accuracy: 0.7560\n",
      "Epoch 84/120\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.9784 - accuracy: 0.8445 - val_loss: 1.1601 - val_accuracy: 0.7560\n",
      "Epoch 85/120\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.9737 - accuracy: 0.8475 - val_loss: 1.1529 - val_accuracy: 0.7670\n",
      "Epoch 86/120\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.9694 - accuracy: 0.8479 - val_loss: 1.1542 - val_accuracy: 0.7590\n",
      "Epoch 87/120\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.9649 - accuracy: 0.8481 - val_loss: 1.1480 - val_accuracy: 0.7630\n",
      "Epoch 88/120\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.9601 - accuracy: 0.8504 - val_loss: 1.1488 - val_accuracy: 0.7610\n",
      "Epoch 89/120\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.9558 - accuracy: 0.8520 - val_loss: 1.1414 - val_accuracy: 0.7650\n",
      "Epoch 90/120\n",
      "30/30 [==============================] - 1s 32ms/step - loss: 0.9517 - accuracy: 0.8520 - val_loss: 1.1426 - val_accuracy: 0.7640\n",
      "Epoch 91/120\n",
      "30/30 [==============================] - 1s 26ms/step - loss: 0.9471 - accuracy: 0.8531 - val_loss: 1.1424 - val_accuracy: 0.7610\n",
      "Epoch 92/120\n",
      "30/30 [==============================] - 1s 27ms/step - loss: 0.9431 - accuracy: 0.8544 - val_loss: 1.1389 - val_accuracy: 0.7640\n",
      "Epoch 93/120\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.9387 - accuracy: 0.8563 - val_loss: 1.1359 - val_accuracy: 0.7570\n",
      "Epoch 94/120\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.9354 - accuracy: 0.8563 - val_loss: 1.1313 - val_accuracy: 0.7680\n",
      "Epoch 95/120\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 0.9304 - accuracy: 0.8557 - val_loss: 1.1264 - val_accuracy: 0.7680\n",
      "Epoch 96/120\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.9265 - accuracy: 0.8569 - val_loss: 1.1277 - val_accuracy: 0.7650\n",
      "Epoch 97/120\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.9225 - accuracy: 0.8580 - val_loss: 1.1262 - val_accuracy: 0.7610\n",
      "Epoch 98/120\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.9184 - accuracy: 0.8591 - val_loss: 1.1230 - val_accuracy: 0.7640\n",
      "Epoch 99/120\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.9144 - accuracy: 0.8599 - val_loss: 1.1225 - val_accuracy: 0.7660\n",
      "Epoch 100/120\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.9110 - accuracy: 0.8623 - val_loss: 1.1245 - val_accuracy: 0.7590\n",
      "Epoch 101/120\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.9066 - accuracy: 0.8607 - val_loss: 1.1209 - val_accuracy: 0.7630\n",
      "Epoch 102/120\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.9027 - accuracy: 0.8635 - val_loss: 1.1172 - val_accuracy: 0.7610\n",
      "Epoch 103/120\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.8988 - accuracy: 0.8641 - val_loss: 1.1116 - val_accuracy: 0.7680\n",
      "Epoch 104/120\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.8952 - accuracy: 0.8656 - val_loss: 1.1110 - val_accuracy: 0.7650\n",
      "Epoch 105/120\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.8915 - accuracy: 0.8660 - val_loss: 1.1104 - val_accuracy: 0.7670\n",
      "Epoch 106/120\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.8875 - accuracy: 0.8675 - val_loss: 1.1101 - val_accuracy: 0.7640\n",
      "Epoch 107/120\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.8840 - accuracy: 0.8709 - val_loss: 1.1068 - val_accuracy: 0.7670\n",
      "Epoch 108/120\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.8803 - accuracy: 0.8688 - val_loss: 1.1075 - val_accuracy: 0.7690\n",
      "Epoch 109/120\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.8765 - accuracy: 0.8687 - val_loss: 1.1073 - val_accuracy: 0.7590\n",
      "Epoch 110/120\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.8736 - accuracy: 0.8703 - val_loss: 1.1034 - val_accuracy: 0.7670\n",
      "Epoch 111/120\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.8694 - accuracy: 0.8705 - val_loss: 1.1008 - val_accuracy: 0.7620\n",
      "Epoch 112/120\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.8665 - accuracy: 0.8715 - val_loss: 1.0995 - val_accuracy: 0.7700\n",
      "Epoch 113/120\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.8628 - accuracy: 0.8732 - val_loss: 1.0933 - val_accuracy: 0.7710\n",
      "Epoch 114/120\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.8590 - accuracy: 0.8755 - val_loss: 1.0911 - val_accuracy: 0.7690\n",
      "Epoch 115/120\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.8556 - accuracy: 0.8739 - val_loss: 1.0996 - val_accuracy: 0.7600\n",
      "Epoch 116/120\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.8525 - accuracy: 0.8741 - val_loss: 1.0924 - val_accuracy: 0.7620\n",
      "Epoch 117/120\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.8491 - accuracy: 0.8761 - val_loss: 1.0908 - val_accuracy: 0.7670\n",
      "Epoch 118/120\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.8457 - accuracy: 0.8757 - val_loss: 1.0937 - val_accuracy: 0.7650\n",
      "Epoch 119/120\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.8421 - accuracy: 0.8773 - val_loss: 1.0880 - val_accuracy: 0.7650\n",
      "Epoch 120/120\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.8390 - accuracy: 0.8795 - val_loss: 1.0836 - val_accuracy: 0.7740\n"
     ]
    }
   ],
   "source": [
    "from keras import regularizers\n",
    "model = Sequential()\n",
    "model.add(Dense(50, activation='relu', input_dim=2000, kernel_regularizer=regularizers.l2(0.005)))\n",
    "model.add(Dense(25, activation='relu', kernel_regularizer=regularizers.l2(0.005)))\n",
    "model.add(Dense(7, activation='softmax'))\n",
    "model.compile(optimizer='SGD', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "L2_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=120,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's see how regularization has affected our model results.  \n",
    "\n",
    "Run the cell below to get the model's `.history`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L2_model_dict = L2_model.history\n",
    "L2_model_dict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the training accuracy as well as the validation accuracy for both the L2 and the model without regularization (for 120 epochs).\n",
    "\n",
    "Run the cell below to visualize our training and validation accuracy both with and without L2 regularization, so that we can compare them directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "\n",
    "acc_values = L2_model_dict['accuracy'] \n",
    "val_acc_values = L2_model_dict['val_accuracy']\n",
    "model_acc = model_val_dict['accuracy']\n",
    "model_val_acc = model_val_dict['val_accuracy']\n",
    "\n",
    "epochs = range(1, len(acc_values) + 1)\n",
    "plt.plot(epochs, acc_values, 'g', label='Training acc L2')\n",
    "plt.plot(epochs, val_acc_values, 'g', label='Validation acc L2')\n",
    "plt.plot(epochs, model_acc, 'r', label='Training acc')\n",
    "plt.plot(epochs, model_val_acc, 'r', label='Validation acc')\n",
    "plt.title('Training & validation accuracy L2 vs regular')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of L2 regularization are quite disappointing here. We notice the discrepancy between validation and training accuracy seems to have decreased slightly, but the end result is definitely not getting better. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.  L1 regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at L1 regularization. Will this work better?\n",
    "\n",
    "In the cell below: \n",
    "\n",
    "* Recreate the same model we did above, but this time, set the `kernel_regularizer` to `regularizers.l1(0.005)` inside both hidden layers. \n",
    "* Compile and fit the model exactly as we did for our L2 Regularization experiment (`120` epochs) \n",
    "* Store the fitted model that the `.fit` call returns inside a variable called `L1_model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/120\n",
      "30/30 [==============================] - 2s 27ms/step - loss: 16.0284 - accuracy: 0.1593 - val_loss: 15.6220 - val_accuracy: 0.1790\n",
      "Epoch 2/120\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 15.2703 - accuracy: 0.1896 - val_loss: 14.8782 - val_accuracy: 0.2100\n",
      "Epoch 3/120\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 14.5355 - accuracy: 0.2117 - val_loss: 14.1558 - val_accuracy: 0.2350\n",
      "Epoch 4/120\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 13.8203 - accuracy: 0.2256 - val_loss: 13.4520 - val_accuracy: 0.2420\n",
      "Epoch 5/120\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 13.1238 - accuracy: 0.2460 - val_loss: 12.7661 - val_accuracy: 0.2610\n",
      "Epoch 6/120\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 12.4449 - accuracy: 0.2707 - val_loss: 12.0982 - val_accuracy: 0.2810\n",
      "Epoch 7/120\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 11.7849 - accuracy: 0.2909 - val_loss: 11.4502 - val_accuracy: 0.2920\n",
      "Epoch 8/120\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 11.1453 - accuracy: 0.3105 - val_loss: 10.8239 - val_accuracy: 0.3070\n",
      "Epoch 9/120\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 10.5269 - accuracy: 0.3256 - val_loss: 10.2183 - val_accuracy: 0.3260\n",
      "Epoch 10/120\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 9.9301 - accuracy: 0.3463 - val_loss: 9.6341 - val_accuracy: 0.3440\n",
      "Epoch 11/120\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 9.3545 - accuracy: 0.3619 - val_loss: 9.0704 - val_accuracy: 0.3620\n",
      "Epoch 12/120\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 8.8000 - accuracy: 0.3829 - val_loss: 8.5284 - val_accuracy: 0.3850\n",
      "Epoch 13/120\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 8.2674 - accuracy: 0.4031 - val_loss: 8.0093 - val_accuracy: 0.4020\n",
      "Epoch 14/120\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 7.7581 - accuracy: 0.4192 - val_loss: 7.5126 - val_accuracy: 0.4120\n",
      "Epoch 15/120\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 7.2714 - accuracy: 0.4312 - val_loss: 7.0391 - val_accuracy: 0.4310\n",
      "Epoch 16/120\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 6.8075 - accuracy: 0.4509 - val_loss: 6.5880 - val_accuracy: 0.4360\n",
      "Epoch 17/120\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 6.3662 - accuracy: 0.4605 - val_loss: 6.1597 - val_accuracy: 0.4520\n",
      "Epoch 18/120\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 5.9473 - accuracy: 0.4739 - val_loss: 5.7535 - val_accuracy: 0.4770\n",
      "Epoch 19/120\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 5.5505 - accuracy: 0.4937 - val_loss: 5.3691 - val_accuracy: 0.4550\n",
      "Epoch 20/120\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 5.1750 - accuracy: 0.4976 - val_loss: 5.0063 - val_accuracy: 0.4930\n",
      "Epoch 21/120\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 4.8225 - accuracy: 0.5167 - val_loss: 4.6686 - val_accuracy: 0.4840\n",
      "Epoch 22/120\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 4.4939 - accuracy: 0.5255 - val_loss: 4.3536 - val_accuracy: 0.5040\n",
      "Epoch 23/120\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 4.1878 - accuracy: 0.5436 - val_loss: 4.0572 - val_accuracy: 0.5370\n",
      "Epoch 24/120\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 3.9039 - accuracy: 0.5631 - val_loss: 3.7855 - val_accuracy: 0.5560\n",
      "Epoch 25/120\n",
      "30/30 [==============================] - 0s 15ms/step - loss: 3.6416 - accuracy: 0.5819 - val_loss: 3.5333 - val_accuracy: 0.5830\n",
      "Epoch 26/120\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 3.4002 - accuracy: 0.6015 - val_loss: 3.3038 - val_accuracy: 0.6040\n",
      "Epoch 27/120\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 3.1812 - accuracy: 0.6123 - val_loss: 3.0973 - val_accuracy: 0.6130\n",
      "Epoch 28/120\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 2.9845 - accuracy: 0.6197 - val_loss: 2.9133 - val_accuracy: 0.5980\n",
      "Epoch 29/120\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 2.8096 - accuracy: 0.6248 - val_loss: 2.7491 - val_accuracy: 0.6100\n",
      "Epoch 30/120\n",
      "30/30 [==============================] - 1s 21ms/step - loss: 2.6560 - accuracy: 0.6259 - val_loss: 2.6066 - val_accuracy: 0.6260\n",
      "Epoch 31/120\n",
      "30/30 [==============================] - 1s 26ms/step - loss: 2.5230 - accuracy: 0.6319 - val_loss: 2.4842 - val_accuracy: 0.6290\n",
      "Epoch 32/120\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 2.4105 - accuracy: 0.6383 - val_loss: 2.3850 - val_accuracy: 0.6290\n",
      "Epoch 33/120\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 2.3180 - accuracy: 0.6407 - val_loss: 2.2993 - val_accuracy: 0.6280\n",
      "Epoch 34/120\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 2.2432 - accuracy: 0.6445 - val_loss: 2.2336 - val_accuracy: 0.6360\n",
      "Epoch 35/120\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 2.1854 - accuracy: 0.6472 - val_loss: 2.1866 - val_accuracy: 0.6370\n",
      "Epoch 36/120\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 2.1433 - accuracy: 0.6491 - val_loss: 2.1488 - val_accuracy: 0.6430\n",
      "Epoch 37/120\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 2.1117 - accuracy: 0.6504 - val_loss: 2.1206 - val_accuracy: 0.6500\n",
      "Epoch 38/120\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 2.0865 - accuracy: 0.6539 - val_loss: 2.0967 - val_accuracy: 0.6530\n",
      "Epoch 39/120\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 2.0642 - accuracy: 0.6557 - val_loss: 2.0761 - val_accuracy: 0.6590\n",
      "Epoch 40/120\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 2.0440 - accuracy: 0.6616 - val_loss: 2.0565 - val_accuracy: 0.6540\n",
      "Epoch 41/120\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 2.0246 - accuracy: 0.6617 - val_loss: 2.0377 - val_accuracy: 0.6580\n",
      "Epoch 42/120\n",
      "30/30 [==============================] - 0s 15ms/step - loss: 2.0067 - accuracy: 0.6663 - val_loss: 2.0199 - val_accuracy: 0.6560\n",
      "Epoch 43/120\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 1.9894 - accuracy: 0.6675 - val_loss: 2.0027 - val_accuracy: 0.6620\n",
      "Epoch 44/120\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 1.9723 - accuracy: 0.6704 - val_loss: 1.9903 - val_accuracy: 0.6640\n",
      "Epoch 45/120\n",
      "30/30 [==============================] - 1s 22ms/step - loss: 1.9562 - accuracy: 0.6723 - val_loss: 1.9711 - val_accuracy: 0.6630\n",
      "Epoch 46/120\n",
      "30/30 [==============================] - 1s 17ms/step - loss: 1.9401 - accuracy: 0.6737 - val_loss: 1.9599 - val_accuracy: 0.6690\n",
      "Epoch 47/120\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 1.9251 - accuracy: 0.6780 - val_loss: 1.9397 - val_accuracy: 0.6650\n",
      "Epoch 48/120\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 1.9095 - accuracy: 0.6763 - val_loss: 1.9261 - val_accuracy: 0.6680\n",
      "Epoch 49/120\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 1.8953 - accuracy: 0.6795 - val_loss: 1.9101 - val_accuracy: 0.6750\n",
      "Epoch 50/120\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 1.8810 - accuracy: 0.6813 - val_loss: 1.8974 - val_accuracy: 0.6810\n",
      "Epoch 51/120\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 1.8669 - accuracy: 0.6811 - val_loss: 1.8853 - val_accuracy: 0.6690\n",
      "Epoch 52/120\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 1.8533 - accuracy: 0.6817 - val_loss: 1.8728 - val_accuracy: 0.6730\n",
      "Epoch 53/120\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 1.8399 - accuracy: 0.6836 - val_loss: 1.8576 - val_accuracy: 0.6730\n",
      "Epoch 54/120\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 1.8269 - accuracy: 0.6837 - val_loss: 1.8437 - val_accuracy: 0.6850\n",
      "Epoch 55/120\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 1.8146 - accuracy: 0.6840 - val_loss: 1.8324 - val_accuracy: 0.6810\n",
      "Epoch 56/120\n",
      "30/30 [==============================] - 0s 15ms/step - loss: 1.8019 - accuracy: 0.6852 - val_loss: 1.8200 - val_accuracy: 0.6810\n",
      "Epoch 57/120\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 1.7900 - accuracy: 0.6860 - val_loss: 1.8091 - val_accuracy: 0.6850\n",
      "Epoch 58/120\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 1.7778 - accuracy: 0.6891 - val_loss: 1.7956 - val_accuracy: 0.6780\n",
      "Epoch 59/120\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 1.7661 - accuracy: 0.6868 - val_loss: 1.7848 - val_accuracy: 0.6830\n",
      "Epoch 60/120\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 1.7545 - accuracy: 0.6892 - val_loss: 1.7786 - val_accuracy: 0.6800\n",
      "Epoch 61/120\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 1.7440 - accuracy: 0.6895 - val_loss: 1.7614 - val_accuracy: 0.6860\n",
      "Epoch 62/120\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 1.7326 - accuracy: 0.6884 - val_loss: 1.7525 - val_accuracy: 0.6790\n",
      "Epoch 63/120\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 1.7221 - accuracy: 0.6921 - val_loss: 1.7401 - val_accuracy: 0.6820\n",
      "Epoch 64/120\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 1.7114 - accuracy: 0.6923 - val_loss: 1.7321 - val_accuracy: 0.6820\n",
      "Epoch 65/120\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 1.7015 - accuracy: 0.6921 - val_loss: 1.7220 - val_accuracy: 0.6880\n",
      "Epoch 66/120\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 1.6910 - accuracy: 0.6931 - val_loss: 1.7157 - val_accuracy: 0.6820\n",
      "Epoch 67/120\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 1.6815 - accuracy: 0.6941 - val_loss: 1.7006 - val_accuracy: 0.6830\n",
      "Epoch 68/120\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 1.6717 - accuracy: 0.6939 - val_loss: 1.6898 - val_accuracy: 0.6850\n",
      "Epoch 69/120\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 1.6621 - accuracy: 0.6956 - val_loss: 1.6828 - val_accuracy: 0.6790\n",
      "Epoch 70/120\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 1.6528 - accuracy: 0.6964 - val_loss: 1.6724 - val_accuracy: 0.6850\n",
      "Epoch 71/120\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 1.6435 - accuracy: 0.6969 - val_loss: 1.6674 - val_accuracy: 0.6800\n",
      "Epoch 72/120\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 1.6347 - accuracy: 0.6976 - val_loss: 1.6557 - val_accuracy: 0.6810\n",
      "Epoch 73/120\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 1.6257 - accuracy: 0.6972 - val_loss: 1.6443 - val_accuracy: 0.6860\n",
      "Epoch 74/120\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 1.6168 - accuracy: 0.6984 - val_loss: 1.6366 - val_accuracy: 0.6880\n",
      "Epoch 75/120\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 1.6087 - accuracy: 0.6969 - val_loss: 1.6299 - val_accuracy: 0.6870\n",
      "Epoch 76/120\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 1.5998 - accuracy: 0.6985 - val_loss: 1.6219 - val_accuracy: 0.6820\n",
      "Epoch 77/120\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 1.5910 - accuracy: 0.7011 - val_loss: 1.6131 - val_accuracy: 0.6870\n",
      "Epoch 78/120\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 1.5831 - accuracy: 0.7001 - val_loss: 1.6118 - val_accuracy: 0.6890\n",
      "Epoch 79/120\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 1.5755 - accuracy: 0.6996 - val_loss: 1.5956 - val_accuracy: 0.6900\n",
      "Epoch 80/120\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 1.5672 - accuracy: 0.6997 - val_loss: 1.5903 - val_accuracy: 0.6880\n",
      "Epoch 81/120\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 1.5593 - accuracy: 0.6999 - val_loss: 1.5838 - val_accuracy: 0.6860\n",
      "Epoch 82/120\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 1.5516 - accuracy: 0.7008 - val_loss: 1.5732 - val_accuracy: 0.6860\n",
      "Epoch 83/120\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 1.5439 - accuracy: 0.7023 - val_loss: 1.5663 - val_accuracy: 0.6920\n",
      "Epoch 84/120\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 1.5365 - accuracy: 0.7025 - val_loss: 1.5600 - val_accuracy: 0.6900\n",
      "Epoch 85/120\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 1.5290 - accuracy: 0.7040 - val_loss: 1.5511 - val_accuracy: 0.6890\n",
      "Epoch 86/120\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 1.5215 - accuracy: 0.7043 - val_loss: 1.5431 - val_accuracy: 0.6900\n",
      "Epoch 87/120\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 1.5144 - accuracy: 0.7047 - val_loss: 1.5387 - val_accuracy: 0.6850\n",
      "Epoch 88/120\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 1.5068 - accuracy: 0.7056 - val_loss: 1.5349 - val_accuracy: 0.6950\n",
      "Epoch 89/120\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 1.4998 - accuracy: 0.7064 - val_loss: 1.5236 - val_accuracy: 0.6930\n",
      "Epoch 90/120\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 1.4927 - accuracy: 0.7068 - val_loss: 1.5206 - val_accuracy: 0.6840\n",
      "Epoch 91/120\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 1.4862 - accuracy: 0.7053 - val_loss: 1.5160 - val_accuracy: 0.6890\n",
      "Epoch 92/120\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 1.4798 - accuracy: 0.7073 - val_loss: 1.5034 - val_accuracy: 0.6980\n",
      "Epoch 93/120\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 1.4728 - accuracy: 0.7081 - val_loss: 1.4994 - val_accuracy: 0.6940\n",
      "Epoch 94/120\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 1.4661 - accuracy: 0.7091 - val_loss: 1.4924 - val_accuracy: 0.6930\n",
      "Epoch 95/120\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 1.4596 - accuracy: 0.7096 - val_loss: 1.4841 - val_accuracy: 0.6950\n",
      "Epoch 96/120\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 1.4527 - accuracy: 0.7113 - val_loss: 1.4818 - val_accuracy: 0.6910\n",
      "Epoch 97/120\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 1.4469 - accuracy: 0.7105 - val_loss: 1.4778 - val_accuracy: 0.6970\n",
      "Epoch 98/120\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 1.4402 - accuracy: 0.7132 - val_loss: 1.4673 - val_accuracy: 0.6920\n",
      "Epoch 99/120\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 1.4342 - accuracy: 0.7127 - val_loss: 1.4614 - val_accuracy: 0.6970\n",
      "Epoch 100/120\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 1.4276 - accuracy: 0.7120 - val_loss: 1.4547 - val_accuracy: 0.6990\n",
      "Epoch 101/120\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 1.4220 - accuracy: 0.7136 - val_loss: 1.4502 - val_accuracy: 0.6930\n",
      "Epoch 102/120\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 1.4157 - accuracy: 0.7124 - val_loss: 1.4464 - val_accuracy: 0.6950\n",
      "Epoch 103/120\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 1.4100 - accuracy: 0.7129 - val_loss: 1.4410 - val_accuracy: 0.6930\n",
      "Epoch 104/120\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 1.4041 - accuracy: 0.7127 - val_loss: 1.4370 - val_accuracy: 0.6970\n",
      "Epoch 105/120\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 1.3982 - accuracy: 0.7177 - val_loss: 1.4287 - val_accuracy: 0.6970\n",
      "Epoch 106/120\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 1.3924 - accuracy: 0.7152 - val_loss: 1.4220 - val_accuracy: 0.6990\n",
      "Epoch 107/120\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 1.3869 - accuracy: 0.7143 - val_loss: 1.4151 - val_accuracy: 0.6990\n",
      "Epoch 108/120\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 1.3813 - accuracy: 0.7167 - val_loss: 1.4088 - val_accuracy: 0.7000\n",
      "Epoch 109/120\n",
      "30/30 [==============================] - 1s 26ms/step - loss: 1.3754 - accuracy: 0.7165 - val_loss: 1.4075 - val_accuracy: 0.6970\n",
      "Epoch 110/120\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 1.3704 - accuracy: 0.7175 - val_loss: 1.3977 - val_accuracy: 0.7000\n",
      "Epoch 111/120\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 1.3648 - accuracy: 0.7183 - val_loss: 1.3966 - val_accuracy: 0.6970\n",
      "Epoch 112/120\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 1.3599 - accuracy: 0.7188 - val_loss: 1.3905 - val_accuracy: 0.6960\n",
      "Epoch 113/120\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 1.3540 - accuracy: 0.7184 - val_loss: 1.3834 - val_accuracy: 0.6990\n",
      "Epoch 114/120\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 1.3488 - accuracy: 0.7200 - val_loss: 1.3805 - val_accuracy: 0.6950\n",
      "Epoch 115/120\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 1.3441 - accuracy: 0.7212 - val_loss: 1.3755 - val_accuracy: 0.6990\n",
      "Epoch 116/120\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 1.3389 - accuracy: 0.7192 - val_loss: 1.3697 - val_accuracy: 0.6980\n",
      "Epoch 117/120\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 1.3338 - accuracy: 0.7211 - val_loss: 1.3660 - val_accuracy: 0.7000\n",
      "Epoch 118/120\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 1.3292 - accuracy: 0.7209 - val_loss: 1.3656 - val_accuracy: 0.6960\n",
      "Epoch 119/120\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 1.3242 - accuracy: 0.7207 - val_loss: 1.3561 - val_accuracy: 0.7010\n",
      "Epoch 120/120\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 1.3192 - accuracy: 0.7223 - val_loss: 1.3557 - val_accuracy: 0.7010\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(50, activation='relu', input_dim=2000, kernel_regularizer=regularizers.l1(0.005)))\n",
    "model.add(Dense(25, activation='relu', kernel_regularizer=regularizers.l1(0.005)))\n",
    "model.add(Dense(7, activation='softmax'))\n",
    "model.compile(optimizer='SGD', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "L1_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=120,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, run the cell below to get and visualize the model's `.history`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "L1_model_dict = L1_model.history\n",
    "plt.clf()\n",
    "\n",
    "acc_values = L1_model_dict['accuracy'] \n",
    "val_acc_values = L1_model_dict['val_accuracy']\n",
    "\n",
    "epochs = range(1, len(acc_values) + 1)\n",
    "plt.plot(epochs, acc_values, 'g', label='Training acc L1')\n",
    "plt.plot(epochs, val_acc_values, 'g.', label='Validation acc L1')\n",
    "plt.title('Training & validation accuracy with L1 regularization')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how The training and validation accuracy don't diverge as much as before! Unfortunately, the validation accuracy doesn't reach rates much higher than 70%. It does seem like we can still improve the model by training much longer.\n",
    "\n",
    "To complete our comparison, let's use `model.evaluate()` again on the appropriate variables to compare results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235/235 [==============================] - 1s 2ms/step - loss: 1.3203 - accuracy: 0.7200\n",
      "47/47 [==============================] - 0s 2ms/step - loss: 1.3621 - accuracy: 0.7067\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)\n",
    "\n",
    "results_test = model.evaluate(test, label_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.320335865020752, 0.7200000286102295]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train # Expected Output: [1.3186310468037923, 0.72266666663487755]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.3620622158050537, 0.7066666483879089]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test # Expected Output: [1.3541648308436076, 0.70800000031789145]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is about the best we've seen so far, but we were training for quite a while! Let's see if dropout regularization can do even better and/or be more efficient!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Dropout Regularization\n",
    "\n",
    "Dropout Regularization is accomplished by adding in an additional `Dropout` layer wherever we want to use it, and providing a percentage value for how likely any given neuron is to get \"dropped out\" during this layer. \n",
    "\n",
    "In the cell below:\n",
    "\n",
    "* Import `Dropout` from `keras.layers`\n",
    "* Recreate the same network we have above, but this time without any L1 or L2 regularization\n",
    "* Add a `Dropout` layer between hidden layer 1 and hidden layer 2.  This should have a dropout chance of `0.3`.\n",
    "* Add a `Dropout` layer between hidden layer 2 and the output layer.  This should have a dropout chance of `0.3`.\n",
    "* Compile the model with the exact same hyperparameters as all other models we've built. \n",
    "* Fit the model with the same hyperparameters we've used above.  But this time, train the model for `200` epochs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "30/30 [==============================] - 1s 17ms/step - loss: 1.9544 - accuracy: 0.1488 - val_loss: 1.9415 - val_accuracy: 0.1540\n",
      "Epoch 2/200\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 1.9411 - accuracy: 0.1621 - val_loss: 1.9329 - val_accuracy: 0.1950\n",
      "Epoch 3/200\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 1.9333 - accuracy: 0.1689 - val_loss: 1.9260 - val_accuracy: 0.2100\n",
      "Epoch 4/200\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 1.9244 - accuracy: 0.1827 - val_loss: 1.9191 - val_accuracy: 0.2180\n",
      "Epoch 5/200\n",
      "30/30 [==============================] - 1s 21ms/step - loss: 1.9150 - accuracy: 0.1921 - val_loss: 1.9115 - val_accuracy: 0.2280\n",
      "Epoch 6/200\n",
      "30/30 [==============================] - 0s 15ms/step - loss: 1.9106 - accuracy: 0.1983 - val_loss: 1.9048 - val_accuracy: 0.2200\n",
      "Epoch 7/200\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 1.9020 - accuracy: 0.2139 - val_loss: 1.8967 - val_accuracy: 0.2200\n",
      "Epoch 8/200\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 1.8960 - accuracy: 0.2191 - val_loss: 1.8874 - val_accuracy: 0.2230\n",
      "Epoch 9/200\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 1.8841 - accuracy: 0.2277 - val_loss: 1.8780 - val_accuracy: 0.2260\n",
      "Epoch 10/200\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 1.8738 - accuracy: 0.2356 - val_loss: 1.8662 - val_accuracy: 0.2280\n",
      "Epoch 11/200\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 1.8625 - accuracy: 0.2428 - val_loss: 1.8533 - val_accuracy: 0.2460\n",
      "Epoch 12/200\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 1.8553 - accuracy: 0.2420 - val_loss: 1.8400 - val_accuracy: 0.2570\n",
      "Epoch 13/200\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 1.8394 - accuracy: 0.2627 - val_loss: 1.8249 - val_accuracy: 0.2590\n",
      "Epoch 14/200\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 1.8287 - accuracy: 0.2601 - val_loss: 1.8082 - val_accuracy: 0.2740\n",
      "Epoch 15/200\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 1.8100 - accuracy: 0.2827 - val_loss: 1.7882 - val_accuracy: 0.2960\n",
      "Epoch 16/200\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 1.7957 - accuracy: 0.2936 - val_loss: 1.7682 - val_accuracy: 0.3140\n",
      "Epoch 17/200\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 1.7796 - accuracy: 0.2957 - val_loss: 1.7464 - val_accuracy: 0.3620\n",
      "Epoch 18/200\n",
      "30/30 [==============================] - 1s 24ms/step - loss: 1.7634 - accuracy: 0.3079 - val_loss: 1.7230 - val_accuracy: 0.3890\n",
      "Epoch 19/200\n",
      "30/30 [==============================] - 1s 19ms/step - loss: 1.7462 - accuracy: 0.3211 - val_loss: 1.6983 - val_accuracy: 0.4150\n",
      "Epoch 20/200\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 1.7175 - accuracy: 0.3368 - val_loss: 1.6705 - val_accuracy: 0.4300\n",
      "Epoch 21/200\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 1.6984 - accuracy: 0.3535 - val_loss: 1.6417 - val_accuracy: 0.4600\n",
      "Epoch 22/200\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 1.6765 - accuracy: 0.3643 - val_loss: 1.6142 - val_accuracy: 0.4710\n",
      "Epoch 23/200\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 1.6549 - accuracy: 0.3773 - val_loss: 1.5855 - val_accuracy: 0.4870\n",
      "Epoch 24/200\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 1.6329 - accuracy: 0.3813 - val_loss: 1.5567 - val_accuracy: 0.5040\n",
      "Epoch 25/200\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 1.6177 - accuracy: 0.3969 - val_loss: 1.5293 - val_accuracy: 0.5200\n",
      "Epoch 26/200\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 1.5857 - accuracy: 0.4004 - val_loss: 1.4987 - val_accuracy: 0.5380\n",
      "Epoch 27/200\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 1.5670 - accuracy: 0.4184 - val_loss: 1.4697 - val_accuracy: 0.5600\n",
      "Epoch 28/200\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 1.5417 - accuracy: 0.4231 - val_loss: 1.4407 - val_accuracy: 0.5740\n",
      "Epoch 29/200\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 1.5184 - accuracy: 0.4317 - val_loss: 1.4116 - val_accuracy: 0.5890\n",
      "Epoch 30/200\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 1.4986 - accuracy: 0.4411 - val_loss: 1.3854 - val_accuracy: 0.5950\n",
      "Epoch 31/200\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 1.4767 - accuracy: 0.4492 - val_loss: 1.3575 - val_accuracy: 0.6070\n",
      "Epoch 32/200\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 1.4439 - accuracy: 0.4625 - val_loss: 1.3279 - val_accuracy: 0.6150\n",
      "Epoch 33/200\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 1.4283 - accuracy: 0.4649 - val_loss: 1.2999 - val_accuracy: 0.6260\n",
      "Epoch 34/200\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 1.4154 - accuracy: 0.4815 - val_loss: 1.2767 - val_accuracy: 0.6340\n",
      "Epoch 35/200\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 1.3889 - accuracy: 0.4945 - val_loss: 1.2515 - val_accuracy: 0.6380\n",
      "Epoch 36/200\n",
      "30/30 [==============================] - 0s 15ms/step - loss: 1.3636 - accuracy: 0.5055 - val_loss: 1.2268 - val_accuracy: 0.6420\n",
      "Epoch 37/200\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 1.3382 - accuracy: 0.5077 - val_loss: 1.2029 - val_accuracy: 0.6510\n",
      "Epoch 38/200\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 1.3212 - accuracy: 0.5183 - val_loss: 1.1762 - val_accuracy: 0.6570\n",
      "Epoch 39/200\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 1.3056 - accuracy: 0.5208 - val_loss: 1.1564 - val_accuracy: 0.6540\n",
      "Epoch 40/200\n",
      "30/30 [==============================] - 0s 15ms/step - loss: 1.2874 - accuracy: 0.5300 - val_loss: 1.1353 - val_accuracy: 0.6610\n",
      "Epoch 41/200\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 1.2745 - accuracy: 0.5331 - val_loss: 1.1156 - val_accuracy: 0.6660\n",
      "Epoch 42/200\n",
      "30/30 [==============================] - 1s 19ms/step - loss: 1.2539 - accuracy: 0.5425 - val_loss: 1.0955 - val_accuracy: 0.6660\n",
      "Epoch 43/200\n",
      "30/30 [==============================] - 1s 27ms/step - loss: 1.2271 - accuracy: 0.5517 - val_loss: 1.0757 - val_accuracy: 0.6720\n",
      "Epoch 44/200\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 1.2092 - accuracy: 0.5615 - val_loss: 1.0578 - val_accuracy: 0.6750\n",
      "Epoch 45/200\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 1.1931 - accuracy: 0.5627 - val_loss: 1.0395 - val_accuracy: 0.6790\n",
      "Epoch 46/200\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 1.1817 - accuracy: 0.5707 - val_loss: 1.0237 - val_accuracy: 0.6910\n",
      "Epoch 47/200\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 1.1578 - accuracy: 0.5769 - val_loss: 1.0060 - val_accuracy: 0.6860\n",
      "Epoch 48/200\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 1.1543 - accuracy: 0.5751 - val_loss: 0.9919 - val_accuracy: 0.6900\n",
      "Epoch 49/200\n",
      "30/30 [==============================] - 0s 16ms/step - loss: 1.1437 - accuracy: 0.5749 - val_loss: 0.9789 - val_accuracy: 0.6870\n",
      "Epoch 50/200\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 1.1385 - accuracy: 0.5783 - val_loss: 0.9660 - val_accuracy: 0.6890\n",
      "Epoch 51/200\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 1.0996 - accuracy: 0.6013 - val_loss: 0.9502 - val_accuracy: 0.6950\n",
      "Epoch 52/200\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 1.0989 - accuracy: 0.5975 - val_loss: 0.9389 - val_accuracy: 0.6930\n",
      "Epoch 53/200\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 1.0849 - accuracy: 0.6011 - val_loss: 0.9265 - val_accuracy: 0.7040\n",
      "Epoch 54/200\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 1.0762 - accuracy: 0.6073 - val_loss: 0.9155 - val_accuracy: 0.7020\n",
      "Epoch 55/200\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 1.0611 - accuracy: 0.6067 - val_loss: 0.9035 - val_accuracy: 0.7020\n",
      "Epoch 56/200\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 1.0600 - accuracy: 0.6087 - val_loss: 0.8955 - val_accuracy: 0.7030\n",
      "Epoch 57/200\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 1.0294 - accuracy: 0.6213 - val_loss: 0.8832 - val_accuracy: 0.7020\n",
      "Epoch 58/200\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 1.0409 - accuracy: 0.6240 - val_loss: 0.8759 - val_accuracy: 0.7070\n",
      "Epoch 59/200\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 1.0184 - accuracy: 0.6251 - val_loss: 0.8648 - val_accuracy: 0.7140\n",
      "Epoch 60/200\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 1.0065 - accuracy: 0.6296 - val_loss: 0.8536 - val_accuracy: 0.7170\n",
      "Epoch 61/200\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 1.0022 - accuracy: 0.6287 - val_loss: 0.8473 - val_accuracy: 0.7170\n",
      "Epoch 62/200\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.9851 - accuracy: 0.6420 - val_loss: 0.8387 - val_accuracy: 0.7150\n",
      "Epoch 63/200\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.9841 - accuracy: 0.6344 - val_loss: 0.8312 - val_accuracy: 0.7170\n",
      "Epoch 64/200\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.9681 - accuracy: 0.6401 - val_loss: 0.8263 - val_accuracy: 0.7180\n",
      "Epoch 65/200\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.9637 - accuracy: 0.6449 - val_loss: 0.8196 - val_accuracy: 0.7180\n",
      "Epoch 66/200\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 0.9621 - accuracy: 0.6456 - val_loss: 0.8148 - val_accuracy: 0.7180\n",
      "Epoch 67/200\n",
      "30/30 [==============================] - 1s 21ms/step - loss: 0.9522 - accuracy: 0.6479 - val_loss: 0.8071 - val_accuracy: 0.7220\n",
      "Epoch 68/200\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.9368 - accuracy: 0.6496 - val_loss: 0.8017 - val_accuracy: 0.7200\n",
      "Epoch 69/200\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.9225 - accuracy: 0.6629 - val_loss: 0.7937 - val_accuracy: 0.7230\n",
      "Epoch 70/200\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.9248 - accuracy: 0.6576 - val_loss: 0.7878 - val_accuracy: 0.7200\n",
      "Epoch 71/200\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.9204 - accuracy: 0.6565 - val_loss: 0.7824 - val_accuracy: 0.7230\n",
      "Epoch 72/200\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.9084 - accuracy: 0.6599 - val_loss: 0.7772 - val_accuracy: 0.7240\n",
      "Epoch 73/200\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.8948 - accuracy: 0.6641 - val_loss: 0.7728 - val_accuracy: 0.7240\n",
      "Epoch 74/200\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.8926 - accuracy: 0.6669 - val_loss: 0.7686 - val_accuracy: 0.7220\n",
      "Epoch 75/200\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.8819 - accuracy: 0.6688 - val_loss: 0.7645 - val_accuracy: 0.7270\n",
      "Epoch 76/200\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.8840 - accuracy: 0.6753 - val_loss: 0.7598 - val_accuracy: 0.7260\n",
      "Epoch 77/200\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.8723 - accuracy: 0.6687 - val_loss: 0.7551 - val_accuracy: 0.7280\n",
      "Epoch 78/200\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.8640 - accuracy: 0.6819 - val_loss: 0.7524 - val_accuracy: 0.7270\n",
      "Epoch 79/200\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.8638 - accuracy: 0.6805 - val_loss: 0.7488 - val_accuracy: 0.7260\n",
      "Epoch 80/200\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.8548 - accuracy: 0.6799 - val_loss: 0.7463 - val_accuracy: 0.7240\n",
      "Epoch 81/200\n",
      "30/30 [==============================] - 1s 23ms/step - loss: 0.8530 - accuracy: 0.6815 - val_loss: 0.7426 - val_accuracy: 0.7270\n",
      "Epoch 82/200\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.8534 - accuracy: 0.6775 - val_loss: 0.7416 - val_accuracy: 0.7290\n",
      "Epoch 83/200\n",
      "30/30 [==============================] - 1s 29ms/step - loss: 0.8482 - accuracy: 0.6813 - val_loss: 0.7369 - val_accuracy: 0.7320\n",
      "Epoch 84/200\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 0.8432 - accuracy: 0.6859 - val_loss: 0.7346 - val_accuracy: 0.7270\n",
      "Epoch 85/200\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.8255 - accuracy: 0.6924 - val_loss: 0.7290 - val_accuracy: 0.7310\n",
      "Epoch 86/200\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.8235 - accuracy: 0.6895 - val_loss: 0.7258 - val_accuracy: 0.7330\n",
      "Epoch 87/200\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.8060 - accuracy: 0.7041 - val_loss: 0.7222 - val_accuracy: 0.7320\n",
      "Epoch 88/200\n",
      "30/30 [==============================] - 1s 36ms/step - loss: 0.8203 - accuracy: 0.6935 - val_loss: 0.7199 - val_accuracy: 0.7320\n",
      "Epoch 89/200\n",
      "30/30 [==============================] - 1s 32ms/step - loss: 0.8043 - accuracy: 0.7040 - val_loss: 0.7171 - val_accuracy: 0.7340\n",
      "Epoch 90/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.8106 - accuracy: 0.6965 - val_loss: 0.7161 - val_accuracy: 0.7330\n",
      "Epoch 91/200\n",
      "30/30 [==============================] - 1s 29ms/step - loss: 0.8004 - accuracy: 0.6995 - val_loss: 0.7137 - val_accuracy: 0.7340\n",
      "Epoch 92/200\n",
      "30/30 [==============================] - 1s 31ms/step - loss: 0.7959 - accuracy: 0.7011 - val_loss: 0.7109 - val_accuracy: 0.7340\n",
      "Epoch 93/200\n",
      "30/30 [==============================] - 1s 26ms/step - loss: 0.7752 - accuracy: 0.7084 - val_loss: 0.7097 - val_accuracy: 0.7320\n",
      "Epoch 94/200\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.7790 - accuracy: 0.7091 - val_loss: 0.7069 - val_accuracy: 0.7360\n",
      "Epoch 95/200\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 0.7867 - accuracy: 0.7065 - val_loss: 0.7046 - val_accuracy: 0.7370\n",
      "Epoch 96/200\n",
      "30/30 [==============================] - 1s 23ms/step - loss: 0.7763 - accuracy: 0.7051 - val_loss: 0.7036 - val_accuracy: 0.7390\n",
      "Epoch 97/200\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.7729 - accuracy: 0.7179 - val_loss: 0.7009 - val_accuracy: 0.7380\n",
      "Epoch 98/200\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 0.7649 - accuracy: 0.7148 - val_loss: 0.6990 - val_accuracy: 0.7370\n",
      "Epoch 99/200\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 0.7655 - accuracy: 0.7151 - val_loss: 0.6972 - val_accuracy: 0.7390\n",
      "Epoch 100/200\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 0.7595 - accuracy: 0.7139 - val_loss: 0.6944 - val_accuracy: 0.7420\n",
      "Epoch 101/200\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 0.7441 - accuracy: 0.7227 - val_loss: 0.6934 - val_accuracy: 0.7400\n",
      "Epoch 102/200\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 0.7556 - accuracy: 0.7207 - val_loss: 0.6916 - val_accuracy: 0.7420\n",
      "Epoch 103/200\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 0.7543 - accuracy: 0.7155 - val_loss: 0.6907 - val_accuracy: 0.7400\n",
      "Epoch 104/200\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 0.7302 - accuracy: 0.7257 - val_loss: 0.6890 - val_accuracy: 0.7420\n",
      "Epoch 105/200\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.7505 - accuracy: 0.7193 - val_loss: 0.6883 - val_accuracy: 0.7380\n",
      "Epoch 106/200\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 0.7361 - accuracy: 0.7263 - val_loss: 0.6863 - val_accuracy: 0.7410\n",
      "Epoch 107/200\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.7212 - accuracy: 0.7216 - val_loss: 0.6848 - val_accuracy: 0.7420\n",
      "Epoch 108/200\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.7386 - accuracy: 0.7179 - val_loss: 0.6834 - val_accuracy: 0.7400\n",
      "Epoch 109/200\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.7182 - accuracy: 0.7327 - val_loss: 0.6821 - val_accuracy: 0.7400\n",
      "Epoch 110/200\n",
      "30/30 [==============================] - 1s 37ms/step - loss: 0.7198 - accuracy: 0.7265 - val_loss: 0.6808 - val_accuracy: 0.7440\n",
      "Epoch 111/200\n",
      "30/30 [==============================] - 1s 37ms/step - loss: 0.7161 - accuracy: 0.7288 - val_loss: 0.6800 - val_accuracy: 0.7450\n",
      "Epoch 112/200\n",
      "30/30 [==============================] - 1s 32ms/step - loss: 0.7120 - accuracy: 0.7333 - val_loss: 0.6787 - val_accuracy: 0.7440\n",
      "Epoch 113/200\n",
      "30/30 [==============================] - 1s 27ms/step - loss: 0.7222 - accuracy: 0.7317 - val_loss: 0.6774 - val_accuracy: 0.7420\n",
      "Epoch 114/200\n",
      "30/30 [==============================] - 1s 35ms/step - loss: 0.7058 - accuracy: 0.7317 - val_loss: 0.6760 - val_accuracy: 0.7430\n",
      "Epoch 115/200\n",
      "30/30 [==============================] - 1s 25ms/step - loss: 0.7145 - accuracy: 0.7279 - val_loss: 0.6759 - val_accuracy: 0.7450\n",
      "Epoch 116/200\n",
      "30/30 [==============================] - 1s 24ms/step - loss: 0.7020 - accuracy: 0.7321 - val_loss: 0.6748 - val_accuracy: 0.7420\n",
      "Epoch 117/200\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 0.6876 - accuracy: 0.7440 - val_loss: 0.6737 - val_accuracy: 0.7410\n",
      "Epoch 118/200\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 0.7011 - accuracy: 0.7357 - val_loss: 0.6726 - val_accuracy: 0.7410\n",
      "Epoch 119/200\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 0.6977 - accuracy: 0.7387 - val_loss: 0.6724 - val_accuracy: 0.7420\n",
      "Epoch 120/200\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 0.6960 - accuracy: 0.7339 - val_loss: 0.6710 - val_accuracy: 0.7410\n",
      "Epoch 121/200\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.6887 - accuracy: 0.7379 - val_loss: 0.6696 - val_accuracy: 0.7400\n",
      "Epoch 122/200\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.6840 - accuracy: 0.7383 - val_loss: 0.6698 - val_accuracy: 0.7430\n",
      "Epoch 123/200\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 0.6823 - accuracy: 0.7459 - val_loss: 0.6683 - val_accuracy: 0.7430\n",
      "Epoch 124/200\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 0.6784 - accuracy: 0.7409 - val_loss: 0.6677 - val_accuracy: 0.7420\n",
      "Epoch 125/200\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.6759 - accuracy: 0.7439 - val_loss: 0.6678 - val_accuracy: 0.7440\n",
      "Epoch 126/200\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.6729 - accuracy: 0.7477 - val_loss: 0.6656 - val_accuracy: 0.7420\n",
      "Epoch 127/200\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.6755 - accuracy: 0.7431 - val_loss: 0.6658 - val_accuracy: 0.7430\n",
      "Epoch 128/200\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 0.6693 - accuracy: 0.7513 - val_loss: 0.6651 - val_accuracy: 0.7420\n",
      "Epoch 129/200\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 0.6735 - accuracy: 0.7472 - val_loss: 0.6640 - val_accuracy: 0.7430\n",
      "Epoch 130/200\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.6572 - accuracy: 0.7435 - val_loss: 0.6625 - val_accuracy: 0.7450\n",
      "Epoch 131/200\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 0.6589 - accuracy: 0.7517 - val_loss: 0.6626 - val_accuracy: 0.7380\n",
      "Epoch 132/200\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.6587 - accuracy: 0.7517 - val_loss: 0.6617 - val_accuracy: 0.7410\n",
      "Epoch 133/200\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.6444 - accuracy: 0.7603 - val_loss: 0.6623 - val_accuracy: 0.7490\n",
      "Epoch 134/200\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.6572 - accuracy: 0.7520 - val_loss: 0.6618 - val_accuracy: 0.7390\n",
      "Epoch 135/200\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.6419 - accuracy: 0.7540 - val_loss: 0.6607 - val_accuracy: 0.7440\n",
      "Epoch 136/200\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 0.6509 - accuracy: 0.7484 - val_loss: 0.6602 - val_accuracy: 0.7420\n",
      "Epoch 137/200\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 0.6535 - accuracy: 0.7479 - val_loss: 0.6585 - val_accuracy: 0.7440\n",
      "Epoch 138/200\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.6361 - accuracy: 0.7609 - val_loss: 0.6576 - val_accuracy: 0.7430\n",
      "Epoch 139/200\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.6426 - accuracy: 0.7547 - val_loss: 0.6590 - val_accuracy: 0.7440\n",
      "Epoch 140/200\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.6390 - accuracy: 0.7552 - val_loss: 0.6584 - val_accuracy: 0.7420\n",
      "Epoch 141/200\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 0.6254 - accuracy: 0.7551 - val_loss: 0.6578 - val_accuracy: 0.7410\n",
      "Epoch 142/200\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 0.6384 - accuracy: 0.7533 - val_loss: 0.6564 - val_accuracy: 0.7430\n",
      "Epoch 143/200\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 0.6285 - accuracy: 0.7611 - val_loss: 0.6552 - val_accuracy: 0.7400\n",
      "Epoch 144/200\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 0.6223 - accuracy: 0.7599 - val_loss: 0.6565 - val_accuracy: 0.7410\n",
      "Epoch 145/200\n",
      "30/30 [==============================] - 0s 16ms/step - loss: 0.6271 - accuracy: 0.7592 - val_loss: 0.6552 - val_accuracy: 0.7400\n",
      "Epoch 146/200\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 0.6235 - accuracy: 0.7599 - val_loss: 0.6557 - val_accuracy: 0.7440\n",
      "Epoch 147/200\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.6252 - accuracy: 0.7617 - val_loss: 0.6542 - val_accuracy: 0.7420\n",
      "Epoch 148/200\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 0.6134 - accuracy: 0.7659 - val_loss: 0.6557 - val_accuracy: 0.7420\n",
      "Epoch 149/200\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.6203 - accuracy: 0.7683 - val_loss: 0.6537 - val_accuracy: 0.7470\n",
      "Epoch 150/200\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.6090 - accuracy: 0.7667 - val_loss: 0.6527 - val_accuracy: 0.7440\n",
      "Epoch 151/200\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.6090 - accuracy: 0.7699 - val_loss: 0.6539 - val_accuracy: 0.7440\n",
      "Epoch 152/200\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.6110 - accuracy: 0.7669 - val_loss: 0.6533 - val_accuracy: 0.7410\n",
      "Epoch 153/200\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 0.6006 - accuracy: 0.7704 - val_loss: 0.6527 - val_accuracy: 0.7410\n",
      "Epoch 154/200\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 0.6090 - accuracy: 0.7695 - val_loss: 0.6524 - val_accuracy: 0.7430\n",
      "Epoch 155/200\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 0.5967 - accuracy: 0.7701 - val_loss: 0.6520 - val_accuracy: 0.7390\n",
      "Epoch 156/200\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 0.6022 - accuracy: 0.7677 - val_loss: 0.6516 - val_accuracy: 0.7470\n",
      "Epoch 157/200\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.6060 - accuracy: 0.7680 - val_loss: 0.6513 - val_accuracy: 0.7470\n",
      "Epoch 158/200\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 0.6007 - accuracy: 0.7696 - val_loss: 0.6524 - val_accuracy: 0.7420\n",
      "Epoch 159/200\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.6009 - accuracy: 0.7700 - val_loss: 0.6512 - val_accuracy: 0.7450\n",
      "Epoch 160/200\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.6002 - accuracy: 0.7661 - val_loss: 0.6532 - val_accuracy: 0.7450\n",
      "Epoch 161/200\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 0.5919 - accuracy: 0.7720 - val_loss: 0.6517 - val_accuracy: 0.7450\n",
      "Epoch 162/200\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 0.5896 - accuracy: 0.7753 - val_loss: 0.6496 - val_accuracy: 0.7420\n",
      "Epoch 163/200\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.5846 - accuracy: 0.7759 - val_loss: 0.6501 - val_accuracy: 0.7410\n",
      "Epoch 164/200\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 0.5939 - accuracy: 0.7669 - val_loss: 0.6513 - val_accuracy: 0.7450\n",
      "Epoch 165/200\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.5847 - accuracy: 0.7688 - val_loss: 0.6522 - val_accuracy: 0.7460\n",
      "Epoch 166/200\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 0.5765 - accuracy: 0.7787 - val_loss: 0.6498 - val_accuracy: 0.7470\n",
      "Epoch 167/200\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 0.5806 - accuracy: 0.7749 - val_loss: 0.6501 - val_accuracy: 0.7500\n",
      "Epoch 168/200\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.5831 - accuracy: 0.7797 - val_loss: 0.6484 - val_accuracy: 0.7470\n",
      "Epoch 169/200\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.5753 - accuracy: 0.7791 - val_loss: 0.6490 - val_accuracy: 0.7450\n",
      "Epoch 170/200\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 0.5686 - accuracy: 0.7796 - val_loss: 0.6501 - val_accuracy: 0.7440\n",
      "Epoch 171/200\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 0.5735 - accuracy: 0.7781 - val_loss: 0.6486 - val_accuracy: 0.7490\n",
      "Epoch 172/200\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 0.5733 - accuracy: 0.7801 - val_loss: 0.6488 - val_accuracy: 0.7470\n",
      "Epoch 173/200\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.5687 - accuracy: 0.7799 - val_loss: 0.6491 - val_accuracy: 0.7450\n",
      "Epoch 174/200\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 0.5718 - accuracy: 0.7773 - val_loss: 0.6487 - val_accuracy: 0.7480\n",
      "Epoch 175/200\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 0.5683 - accuracy: 0.7833 - val_loss: 0.6477 - val_accuracy: 0.7470\n",
      "Epoch 176/200\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 0.5606 - accuracy: 0.7872 - val_loss: 0.6479 - val_accuracy: 0.7470\n",
      "Epoch 177/200\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.5642 - accuracy: 0.7773 - val_loss: 0.6479 - val_accuracy: 0.7490\n",
      "Epoch 178/200\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 0.5611 - accuracy: 0.7879 - val_loss: 0.6474 - val_accuracy: 0.7470\n",
      "Epoch 179/200\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 0.5510 - accuracy: 0.7884 - val_loss: 0.6479 - val_accuracy: 0.7420\n",
      "Epoch 180/200\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 0.5598 - accuracy: 0.7808 - val_loss: 0.6475 - val_accuracy: 0.7450\n",
      "Epoch 181/200\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 0.5586 - accuracy: 0.7784 - val_loss: 0.6499 - val_accuracy: 0.7440\n",
      "Epoch 182/200\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.5555 - accuracy: 0.7808 - val_loss: 0.6465 - val_accuracy: 0.7450\n",
      "Epoch 183/200\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 0.5532 - accuracy: 0.7839 - val_loss: 0.6464 - val_accuracy: 0.7450\n",
      "Epoch 184/200\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 0.5519 - accuracy: 0.7879 - val_loss: 0.6459 - val_accuracy: 0.7480\n",
      "Epoch 185/200\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.5496 - accuracy: 0.7811 - val_loss: 0.6470 - val_accuracy: 0.7430\n",
      "Epoch 186/200\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 0.5431 - accuracy: 0.7851 - val_loss: 0.6475 - val_accuracy: 0.7410\n",
      "Epoch 187/200\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 0.5511 - accuracy: 0.7863 - val_loss: 0.6455 - val_accuracy: 0.7510\n",
      "Epoch 188/200\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.5452 - accuracy: 0.7845 - val_loss: 0.6452 - val_accuracy: 0.7450\n",
      "Epoch 189/200\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 0.5421 - accuracy: 0.7879 - val_loss: 0.6469 - val_accuracy: 0.7420\n",
      "Epoch 190/200\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.5370 - accuracy: 0.7876 - val_loss: 0.6466 - val_accuracy: 0.7440\n",
      "Epoch 191/200\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 0.5394 - accuracy: 0.7856 - val_loss: 0.6474 - val_accuracy: 0.7430\n",
      "Epoch 192/200\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 0.5352 - accuracy: 0.7893 - val_loss: 0.6469 - val_accuracy: 0.7480\n",
      "Epoch 193/200\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.5369 - accuracy: 0.7933 - val_loss: 0.6464 - val_accuracy: 0.7430\n",
      "Epoch 194/200\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 0.5334 - accuracy: 0.7905 - val_loss: 0.6462 - val_accuracy: 0.7480\n",
      "Epoch 195/200\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 0.5229 - accuracy: 0.7933 - val_loss: 0.6460 - val_accuracy: 0.7440\n",
      "Epoch 196/200\n",
      "30/30 [==============================] - 1s 19ms/step - loss: 0.5265 - accuracy: 0.7968 - val_loss: 0.6463 - val_accuracy: 0.7450\n",
      "Epoch 197/200\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.5277 - accuracy: 0.7933 - val_loss: 0.6480 - val_accuracy: 0.7440\n",
      "Epoch 198/200\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 0.5330 - accuracy: 0.7908 - val_loss: 0.6463 - val_accuracy: 0.7470\n",
      "Epoch 199/200\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 0.5164 - accuracy: 0.8008 - val_loss: 0.6460 - val_accuracy: 0.7510\n",
      "Epoch 200/200\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.5161 - accuracy: 0.7996 - val_loss: 0.6474 - val_accuracy: 0.7520\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dropout\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(50, activation='relu', input_dim=2000))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(25, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(7, activation='softmax'))\n",
    "model.compile(optimizer='SGD', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "dropout_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=200,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's check the results from `model.evaluate` to see how this change has affected our training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235/235 [==============================] - 0s 2ms/step - loss: 0.3729 - accuracy: 0.8573\n",
      "47/47 [==============================] - 0s 3ms/step - loss: 0.6143 - accuracy: 0.7573\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)\n",
    "results_test = model.evaluate(test, label_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.3729124963283539, 0.8573333621025085]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train # Expected Results: [0.36925017188787462, 0.88026666666666664]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.614325225353241, 0.7573333382606506]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test # Expected Results: [0.69210424280166627, 0.74333333365122478]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see here that the validation performance has improved again! However, the variance did become higher again, compared to L1-regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.  More Training Data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another solution to high variance is to just get more data. We actually *have* more data, but took a subset of 10,000 units before. Let's now quadruple our data set, and see what happens. Note that we are really just lucky here, and getting more data isn't always possible, but this is a useful exercise in order to understand the power of big data sets.\n",
    "\n",
    "Run the cell below to preprocess our entire dataset, instead of just working with a subset of the data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Bank_complaints.csv')\n",
    "random.seed(123)\n",
    "df = df.sample(40000)\n",
    "df.index = range(40000)\n",
    "product = df[\"Product\"]\n",
    "complaints = df[\"Consumer complaint narrative\"]\n",
    "\n",
    "#one-hot encoding of the complaints\n",
    "tokenizer = Tokenizer(num_words=2000)\n",
    "tokenizer.fit_on_texts(complaints)\n",
    "sequences = tokenizer.texts_to_sequences(complaints)\n",
    "one_hot_results= tokenizer.texts_to_matrix(complaints, mode='binary')\n",
    "word_index = tokenizer.word_index\n",
    "np.shape(one_hot_results)\n",
    "\n",
    "#one-hot encoding of products\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(product)\n",
    "list(le.classes_)\n",
    "product_cat = le.transform(product) \n",
    "product_onehot = to_categorical(product_cat)\n",
    "\n",
    "# train test split\n",
    "test_index = random.sample(range(1,40000), 4000)\n",
    "test = one_hot_results[test_index]\n",
    "train = np.delete(one_hot_results, test_index, 0)\n",
    "label_test = product_onehot[test_index]\n",
    "label_train = np.delete(product_onehot, test_index, 0)\n",
    "\n",
    "#Validation set\n",
    "random.seed(123)\n",
    "val = train[:3000]\n",
    "train_final = train[3000:]\n",
    "label_val = label_train[:3000]\n",
    "label_train_final = label_train[3000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, build the first model that we built, without any regularization or dropout layers included. \n",
    "\n",
    "Train this model for 120 epochs.  All other hyperparameters should stay the same.  Store the fitted model inside of `moredata_model`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/120\n",
      "129/129 [==============================] - 2s 8ms/step - loss: 1.9364 - accuracy: 0.1979 - val_loss: 1.9046 - val_accuracy: 0.2627\n",
      "Epoch 2/120\n",
      "129/129 [==============================] - 1s 5ms/step - loss: 1.8541 - accuracy: 0.2932 - val_loss: 1.7930 - val_accuracy: 0.3393\n",
      "Epoch 3/120\n",
      "129/129 [==============================] - 1s 6ms/step - loss: 1.7108 - accuracy: 0.3906 - val_loss: 1.6196 - val_accuracy: 0.4590\n",
      "Epoch 4/120\n",
      "129/129 [==============================] - 1s 5ms/step - loss: 1.5092 - accuracy: 0.5052 - val_loss: 1.4065 - val_accuracy: 0.5437\n",
      "Epoch 5/120\n",
      "129/129 [==============================] - 1s 5ms/step - loss: 1.2995 - accuracy: 0.5886 - val_loss: 1.2142 - val_accuracy: 0.6053\n",
      "Epoch 6/120\n",
      "129/129 [==============================] - 1s 5ms/step - loss: 1.1243 - accuracy: 0.6408 - val_loss: 1.0664 - val_accuracy: 0.6493\n",
      "Epoch 7/120\n",
      "129/129 [==============================] - 1s 4ms/step - loss: 0.9952 - accuracy: 0.6703 - val_loss: 0.9606 - val_accuracy: 0.6760\n",
      "Epoch 8/120\n",
      "129/129 [==============================] - 1s 4ms/step - loss: 0.9039 - accuracy: 0.6946 - val_loss: 0.8872 - val_accuracy: 0.6900\n",
      "Epoch 9/120\n",
      "129/129 [==============================] - 1s 5ms/step - loss: 0.8380 - accuracy: 0.7111 - val_loss: 0.8325 - val_accuracy: 0.7050\n",
      "Epoch 10/120\n",
      "129/129 [==============================] - 1s 5ms/step - loss: 0.7887 - accuracy: 0.7238 - val_loss: 0.7920 - val_accuracy: 0.7203\n",
      "Epoch 11/120\n",
      "129/129 [==============================] - 1s 5ms/step - loss: 0.7502 - accuracy: 0.7321 - val_loss: 0.7611 - val_accuracy: 0.7240\n",
      "Epoch 12/120\n",
      "129/129 [==============================] - 1s 5ms/step - loss: 0.7197 - accuracy: 0.7397 - val_loss: 0.7348 - val_accuracy: 0.7330\n",
      "Epoch 13/120\n",
      "129/129 [==============================] - 1s 5ms/step - loss: 0.6939 - accuracy: 0.7477 - val_loss: 0.7139 - val_accuracy: 0.7390\n",
      "Epoch 14/120\n",
      "129/129 [==============================] - 1s 5ms/step - loss: 0.6722 - accuracy: 0.7543 - val_loss: 0.6958 - val_accuracy: 0.7520\n",
      "Epoch 15/120\n",
      "129/129 [==============================] - 1s 6ms/step - loss: 0.6534 - accuracy: 0.7601 - val_loss: 0.6801 - val_accuracy: 0.7520\n",
      "Epoch 16/120\n",
      "129/129 [==============================] - 1s 4ms/step - loss: 0.6371 - accuracy: 0.7665 - val_loss: 0.6665 - val_accuracy: 0.7623\n",
      "Epoch 17/120\n",
      "129/129 [==============================] - 1s 4ms/step - loss: 0.6224 - accuracy: 0.7718 - val_loss: 0.6552 - val_accuracy: 0.7657\n",
      "Epoch 18/120\n",
      "129/129 [==============================] - 2s 13ms/step - loss: 0.6094 - accuracy: 0.7753 - val_loss: 0.6450 - val_accuracy: 0.7667\n",
      "Epoch 19/120\n",
      "129/129 [==============================] - 2s 12ms/step - loss: 0.5973 - accuracy: 0.7813 - val_loss: 0.6358 - val_accuracy: 0.7703\n",
      "Epoch 20/120\n",
      "129/129 [==============================] - 2s 12ms/step - loss: 0.5866 - accuracy: 0.7855 - val_loss: 0.6273 - val_accuracy: 0.7737\n",
      "Epoch 21/120\n",
      "129/129 [==============================] - 1s 11ms/step - loss: 0.5769 - accuracy: 0.7886 - val_loss: 0.6189 - val_accuracy: 0.7803\n",
      "Epoch 22/120\n",
      "129/129 [==============================] - 1s 9ms/step - loss: 0.5677 - accuracy: 0.7929 - val_loss: 0.6128 - val_accuracy: 0.7760\n",
      "Epoch 23/120\n",
      "129/129 [==============================] - 1s 9ms/step - loss: 0.5589 - accuracy: 0.7962 - val_loss: 0.6061 - val_accuracy: 0.7830\n",
      "Epoch 24/120\n",
      "129/129 [==============================] - 1s 8ms/step - loss: 0.5508 - accuracy: 0.7991 - val_loss: 0.5990 - val_accuracy: 0.7860\n",
      "Epoch 25/120\n",
      "129/129 [==============================] - 1s 9ms/step - loss: 0.5435 - accuracy: 0.8028 - val_loss: 0.5950 - val_accuracy: 0.7843\n",
      "Epoch 26/120\n",
      "129/129 [==============================] - 1s 8ms/step - loss: 0.5361 - accuracy: 0.8060 - val_loss: 0.5912 - val_accuracy: 0.7880\n",
      "Epoch 27/120\n",
      "129/129 [==============================] - 1s 8ms/step - loss: 0.5296 - accuracy: 0.8092 - val_loss: 0.5852 - val_accuracy: 0.7923\n",
      "Epoch 28/120\n",
      "129/129 [==============================] - 1s 8ms/step - loss: 0.5231 - accuracy: 0.8111 - val_loss: 0.5850 - val_accuracy: 0.7870\n",
      "Epoch 29/120\n",
      "129/129 [==============================] - 1s 9ms/step - loss: 0.5171 - accuracy: 0.8141 - val_loss: 0.5773 - val_accuracy: 0.7960\n",
      "Epoch 30/120\n",
      "129/129 [==============================] - 1s 9ms/step - loss: 0.5114 - accuracy: 0.8160 - val_loss: 0.5739 - val_accuracy: 0.7953\n",
      "Epoch 31/120\n",
      "129/129 [==============================] - 1s 8ms/step - loss: 0.5060 - accuracy: 0.8178 - val_loss: 0.5706 - val_accuracy: 0.7980\n",
      "Epoch 32/120\n",
      "129/129 [==============================] - 1s 8ms/step - loss: 0.5007 - accuracy: 0.8199 - val_loss: 0.5683 - val_accuracy: 0.7983\n",
      "Epoch 33/120\n",
      "129/129 [==============================] - 2s 14ms/step - loss: 0.4959 - accuracy: 0.8215 - val_loss: 0.5648 - val_accuracy: 0.8007\n",
      "Epoch 34/120\n",
      "129/129 [==============================] - 1s 11ms/step - loss: 0.4908 - accuracy: 0.8229 - val_loss: 0.5616 - val_accuracy: 0.8030\n",
      "Epoch 35/120\n",
      "129/129 [==============================] - 1s 9ms/step - loss: 0.4862 - accuracy: 0.8258 - val_loss: 0.5587 - val_accuracy: 0.8037\n",
      "Epoch 36/120\n",
      "129/129 [==============================] - 1s 10ms/step - loss: 0.4818 - accuracy: 0.8280 - val_loss: 0.5576 - val_accuracy: 0.8043\n",
      "Epoch 37/120\n",
      "129/129 [==============================] - 1s 10ms/step - loss: 0.4777 - accuracy: 0.8292 - val_loss: 0.5556 - val_accuracy: 0.8060\n",
      "Epoch 38/120\n",
      "129/129 [==============================] - 1s 10ms/step - loss: 0.4732 - accuracy: 0.8304 - val_loss: 0.5556 - val_accuracy: 0.8040\n",
      "Epoch 39/120\n",
      "129/129 [==============================] - 1s 9ms/step - loss: 0.4693 - accuracy: 0.8323 - val_loss: 0.5511 - val_accuracy: 0.8083\n",
      "Epoch 40/120\n",
      "129/129 [==============================] - 1s 10ms/step - loss: 0.4655 - accuracy: 0.8326 - val_loss: 0.5485 - val_accuracy: 0.8077\n",
      "Epoch 41/120\n",
      "129/129 [==============================] - 1s 10ms/step - loss: 0.4614 - accuracy: 0.8343 - val_loss: 0.5475 - val_accuracy: 0.8133\n",
      "Epoch 42/120\n",
      "129/129 [==============================] - 2s 12ms/step - loss: 0.4579 - accuracy: 0.8355 - val_loss: 0.5461 - val_accuracy: 0.8103\n",
      "Epoch 43/120\n",
      "129/129 [==============================] - 1s 10ms/step - loss: 0.4544 - accuracy: 0.8380 - val_loss: 0.5455 - val_accuracy: 0.8103\n",
      "Epoch 44/120\n",
      "129/129 [==============================] - 1s 10ms/step - loss: 0.4509 - accuracy: 0.8386 - val_loss: 0.5437 - val_accuracy: 0.8110\n",
      "Epoch 45/120\n",
      "129/129 [==============================] - 1s 10ms/step - loss: 0.4475 - accuracy: 0.8394 - val_loss: 0.5432 - val_accuracy: 0.8107\n",
      "Epoch 46/120\n",
      "129/129 [==============================] - 1s 10ms/step - loss: 0.4443 - accuracy: 0.8415 - val_loss: 0.5415 - val_accuracy: 0.8137\n",
      "Epoch 47/120\n",
      "129/129 [==============================] - 1s 10ms/step - loss: 0.4409 - accuracy: 0.8420 - val_loss: 0.5417 - val_accuracy: 0.8100\n",
      "Epoch 48/120\n",
      "129/129 [==============================] - 1s 10ms/step - loss: 0.4382 - accuracy: 0.8433 - val_loss: 0.5397 - val_accuracy: 0.8170\n",
      "Epoch 49/120\n",
      "129/129 [==============================] - 1s 11ms/step - loss: 0.4350 - accuracy: 0.8448 - val_loss: 0.5383 - val_accuracy: 0.8170\n",
      "Epoch 50/120\n",
      "129/129 [==============================] - 1s 10ms/step - loss: 0.4319 - accuracy: 0.8465 - val_loss: 0.5393 - val_accuracy: 0.8173\n",
      "Epoch 51/120\n",
      "129/129 [==============================] - 1s 10ms/step - loss: 0.4292 - accuracy: 0.8464 - val_loss: 0.5376 - val_accuracy: 0.8150\n",
      "Epoch 52/120\n",
      "129/129 [==============================] - 1s 10ms/step - loss: 0.4262 - accuracy: 0.8488 - val_loss: 0.5392 - val_accuracy: 0.8163\n",
      "Epoch 53/120\n",
      "129/129 [==============================] - 1s 11ms/step - loss: 0.4233 - accuracy: 0.8495 - val_loss: 0.5372 - val_accuracy: 0.8170\n",
      "Epoch 54/120\n",
      "129/129 [==============================] - 3s 22ms/step - loss: 0.4211 - accuracy: 0.8498 - val_loss: 0.5332 - val_accuracy: 0.8197\n",
      "Epoch 55/120\n",
      "129/129 [==============================] - 2s 19ms/step - loss: 0.4184 - accuracy: 0.8517 - val_loss: 0.5366 - val_accuracy: 0.8180\n",
      "Epoch 56/120\n",
      "129/129 [==============================] - 1s 10ms/step - loss: 0.4156 - accuracy: 0.8518 - val_loss: 0.5327 - val_accuracy: 0.8177\n",
      "Epoch 57/120\n",
      "129/129 [==============================] - 1s 11ms/step - loss: 0.4130 - accuracy: 0.8536 - val_loss: 0.5312 - val_accuracy: 0.8213\n",
      "Epoch 58/120\n",
      "129/129 [==============================] - 1s 10ms/step - loss: 0.4110 - accuracy: 0.8541 - val_loss: 0.5320 - val_accuracy: 0.8197\n",
      "Epoch 59/120\n",
      "129/129 [==============================] - 1s 10ms/step - loss: 0.4082 - accuracy: 0.8550 - val_loss: 0.5318 - val_accuracy: 0.8210\n",
      "Epoch 60/120\n",
      "129/129 [==============================] - 1s 10ms/step - loss: 0.4061 - accuracy: 0.8557 - val_loss: 0.5315 - val_accuracy: 0.8200\n",
      "Epoch 61/120\n",
      "129/129 [==============================] - 1s 10ms/step - loss: 0.4037 - accuracy: 0.8570 - val_loss: 0.5354 - val_accuracy: 0.8183\n",
      "Epoch 62/120\n",
      "129/129 [==============================] - 2s 12ms/step - loss: 0.4016 - accuracy: 0.8575 - val_loss: 0.5306 - val_accuracy: 0.8200\n",
      "Epoch 63/120\n",
      "129/129 [==============================] - 1s 11ms/step - loss: 0.3993 - accuracy: 0.8584 - val_loss: 0.5354 - val_accuracy: 0.8183\n",
      "Epoch 64/120\n",
      "129/129 [==============================] - 2s 12ms/step - loss: 0.3972 - accuracy: 0.8593 - val_loss: 0.5320 - val_accuracy: 0.8200\n",
      "Epoch 65/120\n",
      "129/129 [==============================] - 2s 13ms/step - loss: 0.3950 - accuracy: 0.8605 - val_loss: 0.5318 - val_accuracy: 0.8197\n",
      "Epoch 66/120\n",
      "129/129 [==============================] - 1s 11ms/step - loss: 0.3930 - accuracy: 0.8605 - val_loss: 0.5306 - val_accuracy: 0.8213\n",
      "Epoch 67/120\n",
      "129/129 [==============================] - 1s 11ms/step - loss: 0.3907 - accuracy: 0.8621 - val_loss: 0.5299 - val_accuracy: 0.8183\n",
      "Epoch 68/120\n",
      "129/129 [==============================] - 2s 12ms/step - loss: 0.3887 - accuracy: 0.8627 - val_loss: 0.5313 - val_accuracy: 0.8230\n",
      "Epoch 69/120\n",
      "129/129 [==============================] - 1s 11ms/step - loss: 0.3868 - accuracy: 0.8634 - val_loss: 0.5370 - val_accuracy: 0.8190\n",
      "Epoch 70/120\n",
      "129/129 [==============================] - 1s 11ms/step - loss: 0.3849 - accuracy: 0.8637 - val_loss: 0.5298 - val_accuracy: 0.8207\n",
      "Epoch 71/120\n",
      "129/129 [==============================] - 1s 11ms/step - loss: 0.3831 - accuracy: 0.8642 - val_loss: 0.5330 - val_accuracy: 0.8237\n",
      "Epoch 72/120\n",
      "129/129 [==============================] - 1s 11ms/step - loss: 0.3809 - accuracy: 0.8655 - val_loss: 0.5332 - val_accuracy: 0.8230\n",
      "Epoch 73/120\n",
      "129/129 [==============================] - 2s 12ms/step - loss: 0.3794 - accuracy: 0.8662 - val_loss: 0.5323 - val_accuracy: 0.8230\n",
      "Epoch 74/120\n",
      "129/129 [==============================] - 1s 11ms/step - loss: 0.3772 - accuracy: 0.8664 - val_loss: 0.5330 - val_accuracy: 0.8213\n",
      "Epoch 75/120\n",
      "129/129 [==============================] - 1s 11ms/step - loss: 0.3756 - accuracy: 0.8672 - val_loss: 0.5347 - val_accuracy: 0.8210\n",
      "Epoch 76/120\n",
      "129/129 [==============================] - 1s 11ms/step - loss: 0.3736 - accuracy: 0.8681 - val_loss: 0.5336 - val_accuracy: 0.8223\n",
      "Epoch 77/120\n",
      "129/129 [==============================] - 1s 11ms/step - loss: 0.3719 - accuracy: 0.8688 - val_loss: 0.5348 - val_accuracy: 0.8253\n",
      "Epoch 78/120\n",
      "129/129 [==============================] - 1s 10ms/step - loss: 0.3703 - accuracy: 0.8698 - val_loss: 0.5346 - val_accuracy: 0.8247\n",
      "Epoch 79/120\n",
      "129/129 [==============================] - 1s 10ms/step - loss: 0.3684 - accuracy: 0.8702 - val_loss: 0.5353 - val_accuracy: 0.8203\n",
      "Epoch 80/120\n",
      "129/129 [==============================] - 1s 11ms/step - loss: 0.3670 - accuracy: 0.8705 - val_loss: 0.5355 - val_accuracy: 0.8227\n",
      "Epoch 81/120\n",
      "129/129 [==============================] - 1s 11ms/step - loss: 0.3654 - accuracy: 0.8712 - val_loss: 0.5317 - val_accuracy: 0.8223\n",
      "Epoch 82/120\n",
      "129/129 [==============================] - 1s 11ms/step - loss: 0.3635 - accuracy: 0.8727 - val_loss: 0.5340 - val_accuracy: 0.8237\n",
      "Epoch 83/120\n",
      "129/129 [==============================] - 2s 15ms/step - loss: 0.3619 - accuracy: 0.8727 - val_loss: 0.5331 - val_accuracy: 0.8217\n",
      "Epoch 84/120\n",
      "129/129 [==============================] - 1s 11ms/step - loss: 0.3601 - accuracy: 0.8734 - val_loss: 0.5354 - val_accuracy: 0.8223\n",
      "Epoch 85/120\n",
      "129/129 [==============================] - 1s 11ms/step - loss: 0.3588 - accuracy: 0.8742 - val_loss: 0.5367 - val_accuracy: 0.8220\n",
      "Epoch 86/120\n",
      "129/129 [==============================] - 1s 12ms/step - loss: 0.3571 - accuracy: 0.8742 - val_loss: 0.5359 - val_accuracy: 0.8193\n",
      "Epoch 87/120\n",
      "129/129 [==============================] - 1s 11ms/step - loss: 0.3557 - accuracy: 0.8750 - val_loss: 0.5361 - val_accuracy: 0.8240\n",
      "Epoch 88/120\n",
      "129/129 [==============================] - 2s 15ms/step - loss: 0.3542 - accuracy: 0.8755 - val_loss: 0.5386 - val_accuracy: 0.8207\n",
      "Epoch 89/120\n",
      "129/129 [==============================] - 1s 11ms/step - loss: 0.3524 - accuracy: 0.8767 - val_loss: 0.5397 - val_accuracy: 0.8230\n",
      "Epoch 90/120\n",
      "129/129 [==============================] - 1s 11ms/step - loss: 0.3511 - accuracy: 0.8768 - val_loss: 0.5416 - val_accuracy: 0.8237\n",
      "Epoch 91/120\n",
      "129/129 [==============================] - 1s 11ms/step - loss: 0.3496 - accuracy: 0.8782 - val_loss: 0.5396 - val_accuracy: 0.8213\n",
      "Epoch 92/120\n",
      "129/129 [==============================] - 1s 11ms/step - loss: 0.3481 - accuracy: 0.8783 - val_loss: 0.5416 - val_accuracy: 0.8210\n",
      "Epoch 93/120\n",
      "129/129 [==============================] - 2s 12ms/step - loss: 0.3464 - accuracy: 0.8789 - val_loss: 0.5395 - val_accuracy: 0.8217\n",
      "Epoch 94/120\n",
      "129/129 [==============================] - 1s 12ms/step - loss: 0.3453 - accuracy: 0.8785 - val_loss: 0.5411 - val_accuracy: 0.8170\n",
      "Epoch 95/120\n",
      "129/129 [==============================] - 1s 11ms/step - loss: 0.3437 - accuracy: 0.8797 - val_loss: 0.5490 - val_accuracy: 0.8200\n",
      "Epoch 96/120\n",
      "129/129 [==============================] - 2s 13ms/step - loss: 0.3425 - accuracy: 0.8808 - val_loss: 0.5419 - val_accuracy: 0.8197\n",
      "Epoch 97/120\n",
      "129/129 [==============================] - 1s 10ms/step - loss: 0.3409 - accuracy: 0.8808 - val_loss: 0.5418 - val_accuracy: 0.8217\n",
      "Epoch 98/120\n",
      "129/129 [==============================] - 1s 10ms/step - loss: 0.3397 - accuracy: 0.8808 - val_loss: 0.5428 - val_accuracy: 0.8193\n",
      "Epoch 99/120\n",
      "129/129 [==============================] - 1s 10ms/step - loss: 0.3382 - accuracy: 0.8819 - val_loss: 0.5482 - val_accuracy: 0.8230\n",
      "Epoch 100/120\n",
      "129/129 [==============================] - 1s 10ms/step - loss: 0.3371 - accuracy: 0.8814 - val_loss: 0.5454 - val_accuracy: 0.8207\n",
      "Epoch 101/120\n",
      "129/129 [==============================] - 1s 10ms/step - loss: 0.3357 - accuracy: 0.8830 - val_loss: 0.5480 - val_accuracy: 0.8200\n",
      "Epoch 102/120\n",
      "129/129 [==============================] - 1s 10ms/step - loss: 0.3343 - accuracy: 0.8835 - val_loss: 0.5480 - val_accuracy: 0.8183\n",
      "Epoch 103/120\n",
      "129/129 [==============================] - 1s 10ms/step - loss: 0.3332 - accuracy: 0.8836 - val_loss: 0.5496 - val_accuracy: 0.8213\n",
      "Epoch 104/120\n",
      "129/129 [==============================] - 1s 11ms/step - loss: 0.3316 - accuracy: 0.8840 - val_loss: 0.5484 - val_accuracy: 0.8187\n",
      "Epoch 105/120\n",
      "129/129 [==============================] - 1s 10ms/step - loss: 0.3305 - accuracy: 0.8841 - val_loss: 0.5490 - val_accuracy: 0.8190\n",
      "Epoch 106/120\n",
      "129/129 [==============================] - 1s 10ms/step - loss: 0.3293 - accuracy: 0.8860 - val_loss: 0.5518 - val_accuracy: 0.8207\n",
      "Epoch 107/120\n",
      "129/129 [==============================] - 1s 11ms/step - loss: 0.3275 - accuracy: 0.8854 - val_loss: 0.5493 - val_accuracy: 0.8217\n",
      "Epoch 108/120\n",
      "129/129 [==============================] - 1s 11ms/step - loss: 0.3263 - accuracy: 0.8865 - val_loss: 0.5505 - val_accuracy: 0.8183\n",
      "Epoch 109/120\n",
      "129/129 [==============================] - 2s 12ms/step - loss: 0.3254 - accuracy: 0.8872 - val_loss: 0.5543 - val_accuracy: 0.8187\n",
      "Epoch 110/120\n",
      "129/129 [==============================] - 1s 11ms/step - loss: 0.3241 - accuracy: 0.8868 - val_loss: 0.5530 - val_accuracy: 0.8183\n",
      "Epoch 111/120\n",
      "129/129 [==============================] - 1s 11ms/step - loss: 0.3230 - accuracy: 0.8884 - val_loss: 0.5575 - val_accuracy: 0.8207\n",
      "Epoch 112/120\n",
      "129/129 [==============================] - 2s 13ms/step - loss: 0.3220 - accuracy: 0.8885 - val_loss: 0.5561 - val_accuracy: 0.8190\n",
      "Epoch 113/120\n",
      "129/129 [==============================] - 2s 12ms/step - loss: 0.3206 - accuracy: 0.8879 - val_loss: 0.5567 - val_accuracy: 0.8187\n",
      "Epoch 114/120\n",
      "129/129 [==============================] - 1s 12ms/step - loss: 0.3192 - accuracy: 0.8891 - val_loss: 0.5587 - val_accuracy: 0.8160\n",
      "Epoch 115/120\n",
      "129/129 [==============================] - 1s 11ms/step - loss: 0.3183 - accuracy: 0.8891 - val_loss: 0.5573 - val_accuracy: 0.8190\n",
      "Epoch 116/120\n",
      "129/129 [==============================] - 2s 13ms/step - loss: 0.3167 - accuracy: 0.8900 - val_loss: 0.5588 - val_accuracy: 0.8180\n",
      "Epoch 117/120\n",
      "129/129 [==============================] - 2s 12ms/step - loss: 0.3158 - accuracy: 0.8900 - val_loss: 0.5589 - val_accuracy: 0.8187\n",
      "Epoch 118/120\n",
      "129/129 [==============================] - 2s 12ms/step - loss: 0.3146 - accuracy: 0.8900 - val_loss: 0.5607 - val_accuracy: 0.8193\n",
      "Epoch 119/120\n",
      "129/129 [==============================] - 2s 14ms/step - loss: 0.3135 - accuracy: 0.8896 - val_loss: 0.5598 - val_accuracy: 0.8203\n",
      "Epoch 120/120\n",
      "129/129 [==============================] - 1s 10ms/step - loss: 0.3124 - accuracy: 0.8910 - val_loss: 0.5627 - val_accuracy: 0.8180\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(50, activation='relu', input_dim=2000))\n",
    "model.add(Dense(25, activation='relu'))\n",
    "model.add(Dense(7, activation='softmax'))\n",
    "model.compile(optimizer='SGD', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "moredata_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=120,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, finally, let's check the results returned from `model.evaluate()` to see how this model stacks up with the other techniques we've used.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1032/1032 [==============================] - 3s 2ms/step - loss: 0.3095 - accuracy: 0.8934\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.5614 - accuracy: 0.8045\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)\n",
    "results_test = model.evaluate(test, label_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.3095322549343109, 0.8933939337730408]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train # Expected Output:  [0.31160746300942971, 0.89160606060606062]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5613914728164673, 0.8044999837875366]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test # Expected Output: [0.56076071488857271, 0.8145]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the same amount of epochs, we were able to get a fairly similar validation accuracy of 89.1%. Our test set accuracy went up from ~75% to a staggering 81.45% though, without any other regularization technique. You can still consider early stopping, L1, L2 and dropout here. It's clear that having more data has a strong impact on model performance!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/susanli2016/Machine-Learning-with-Python/blob/master/Consumer_complaints.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://catalog.data.gov/dataset/consumer-complaint-database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further reading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://machinelearningmastery.com/dropout-regularization-deep-learning-models-keras/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
